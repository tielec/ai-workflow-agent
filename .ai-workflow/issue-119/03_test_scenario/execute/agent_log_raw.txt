{"type":"thread.started","thread_id":"019a47b1-6f06-74f3-b9ec-85dd0f0faa78"}
{"type":"turn.started"}
{"type":"item.started","item":{"id":"item_0","type":"todo_list","items":[{"text":"Review planning, requirements, and design documents to understand scope and test strategy","completed":false},{"text":"Derive unit and integration test scenarios aligned with requirements and design","completed":false},{"text":"Draft and save test scenario markdown document at .ai-workflow/issue-119/03_test_scenario/output/test-scenario.md","completed":false}]}}
{"type":"item.started","item":{"id":"item_1","type":"command_execution","command":"bash -lc 'cat .ai-workflow/issue-119/00_planning/output/planning.md'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_1","type":"command_execution","command":"bash -lc 'cat .ai-workflow/issue-119/00_planning/output/planning.md'","aggregated_output":"# プロジェクト計画書 - Issue #119 フォローアップIssue生成品質の改善（LLM活用）\n\n## 1. Issue分析\n- **複雑度**: 複雑  \n  - 新規AI連携モジュールの追加と既存GitHub連携コードの拡張を伴い、外部API統合・エラーハンドリング・フォールバック設計が必要。\n  - テストレイヤーが複数（LLM呼び出しのモック検証と実APIを用いた統合テスト）に渡る。\n- **見積もり工数**: 22~26時間  \n  - 設計/要件整理: 6~7h、実装: 7~9h、テスト（設計＋実装＋実行）: 6~7h、ドキュメント/レポート: 3h を想定。\n- **リスク評価**: 中  \n  - LLM APIの不確定要素と生成品質の主観評価が残る一方、既存コードへの影響は限定範囲に収まる見込み。\n\n## 2. 実装戦略判断\n- **実装戦略**: EXTEND  \n  - 既存の `issue-client.ts` を中心に機能拡張し、新たな `issue-ai-generator.ts` を追加してL TM生成ロジックを組み込む。全体構造は維持したまま責務分割を拡張。\n- **テスト戦略**: UNIT_INTEGRATION  \n  - プロンプト生成・フォールバック制御はモックを使ったユニットテストで網羅し、実API呼び出しは環境変数制御下で統合テストを追加して品質を確認。\n- **テストコード戦略**: BOTH_TEST  \n  - 既存フォローアップ生成ロジックのテストを拡張しつつ、新規 `issue-ai-generator` 用の専用テストファイルを新設する必要がある。\n\n## 3. 影響範囲分析\n- **既存コードへの影響**  \n  - `src/core/github/issue-client.ts`: LLM優先フロー追加、フォールバック制御、ログ出力変更。  \n  - `src/types.ts`: 新しいオプションインターフェースとIssue生成データ構造の拡張。  \n  - `src/commands/execute/agent-setup.ts` などのクライアント初期化部: LLM設定引き回しが必要な場合は拡張。\n- **依存関係の変更**  \n  - 新規AIクライアント実装に伴う依存ライブラリ（公式SDK、HTTPクライアント）の追加検討。  \n  - `.env` や設定ファイルにAPIキー/モデル指定を追加する可能性。\n- **マイグレーション要否**  \n  - コード上のマイグレーションは不要。  \n  - 設定ファイル・ドキュメントへの追記（APIキー設定、プロンプトファイル）を行う。  \n  - 将来的な `.ai-workflow/config.yml` 拡張を見据えた設計が必要。\n\n## 4. タスク分割\n\n### Phase 1: 要件定義 (見積もり: 3~4h)\n- [x] Task 1-1: 現行フォローアップIssue生成フローの分析 (1~1.5h)\n  - `issue-client.ts` のタイトル/本文生成ロジックをシーケンス図レベルで整理\n  - Evaluation Phase から渡る `RemainingTask` / `IssueContext` のデータフローを確認\n  - レビュー結果: 要件定義書にシーケンス分析とデータフロー整理が反映されたため完了\n- [x] Task 1-2: LLM統合要件の明確化 (1.5~2h)\n  - API利用要件（モデル、トークン制限、リトライ戦略）を洗い出す\n  - 生成物の品質条件・受け入れ基準を仕様として文書化\n  - レビュー結果: モデル候補・トークン制限・リトライ戦略が要件定義書に詳細化されたため完了\n\n### Phase 2: 設計 (見積もり: 4~5h)\n- [x] Task 2-1: issue-ai-generatorモジュール設計 (2~2.5h)\n  - クラス/関数責務、依存注入方法、フォールバックパスを設計\n  - プロンプトテンプレートとレスポンス検証手順を定義\n- [x] Task 2-2: 設定・エラーハンドリング設計 (2~2.5h)\n  - API鍵の取得経路とマスキング方針を決定\n  - レート制限、タイムアウト、再試行ポリシーの設計\n\n### Phase 3: テストシナリオ (見積もり: 2~3h)\n- [ ] Task 3-1: テストケース設計 (2~3h)\n  - ユニットテスト（成功/失敗/フォールバック/プロンプト検証）のケースを網羅化\n  - 統合テストで確認すべきシナリオ（実API、環境変数制御、レスポンス妥当性）を整理\n\n### Phase 4: 実装 (見積もり: 6~7h)\n- [ ] Task 4-1: issue-ai-generator.ts の実装 (2.5~3h)\n  - プロンプト生成、API呼び出し、レスポンス整形、エラーハンドリングを実装\n  - ログとトレース情報（入力長、エラー理由）を追加\n- [ ] Task 4-2: issue-client.ts のLLM統合 (2~2.5h)\n  - 新モジュール呼び出しとフォールバック制御を実装\n  - LLM無効化オプションやテレメトリ用ログを追加\n- [ ] Task 4-3: 型/設定まわりの拡張 (1.5~2h)\n  - `types.ts` と設定ファイルにオプションを追加\n  - エージェント初期化・CLIオプションでのフラグ引き回しを反映\n\n### Phase 5: テストコード実装 (見積もり: 4~5h)\n- [ ] Task 5-1: ユニットテスト実装 (2~2.5h)\n  - issue-ai-generator用モックテストを作成\n  - フォールバック動作とプロンプト生成の検証テストを追加\n- [ ] Task 5-2: 統合テスト準備・実装 (2~2.5h)\n  - 実API利用テストを環境変数制御で実行できるよう整備\n  - テストデータ、レート制限保護の仕組み（スキップ条件等）を実装\n\n### Phase 6: テスト実行 (見積もり: 2~3h)\n- [ ] Task 6-1: テスト実行と結果分析 (2~3h)\n  - ユニット/統合テストを実行し、失敗ケースを分析\n  - ログと生成結果をサンプルIssueと比較して品質を確認\n\n### Phase 7: ドキュメント (見積もり: 2h)\n- [ ] Task 7-1: ドキュメント更新 (2h)\n  - `ARCHITECTURE.md`, `CLAUDE.md` にLLM統合方針・利用手順を追記\n  - APIキー設定手順や設定例をREADME系ドキュメントに反映\n\n### Phase 8: レポート (見積もり: 1~1.5h)\n- [ ] Task 8-1: 成果物サマリーとフォローアップ (1~1.5h)\n  - 実装差分、テスト結果、残課題をレポートにまとめる\n  - 次フェーズ向けの改善案や運用上の注意点を記録\n\n## 5. 依存関係\n```mermaid\ngraph TD\n    P1[Phase 1 要件定義] --> P2[Phase 2 設計]\n    P2 --> P3[Phase 3 テストシナリオ]\n    P2 --> P4[Phase 4 実装]\n    P3 --> P5[Phase 5 テストコード実装]\n    P4 --> P5\n    P5 --> P6[Phase 6 テスト実行]\n    P4 --> P7[Phase 7 ドキュメント]\n    P6 --> P8[Phase 8 レポート]\n    P7 --> P8\n```\n\n## 6. リスクと軽減策\n#### リスク1: LLM APIのレート制限・エラー多発\n- **影響度**: 中\n- **確率**: 中\n- **軽減策**: リトライ/指数バックオフを実装し、フォールバックを即時に発動できるようにする。統合テストではAPI呼び出し頻度を制限。\n\n#### リスク2: 生成タイトル・本文の品質ばらつき\n- **影響度**: 高\n- **確率**: 中\n- **軽減策**: プロンプトテンプレートをチューニングし、ユニットテストで最低限のセクション検証を実施。レビュー基準を明文化。\n\n#### リスク3: 機密情報の誤送信\n- **影響度**: 高\n- **確率**: 低\n- **軽減策**: フィルタリングレイヤーを設け、送信前にセンシティブなフィールドを除去。APIログに機密情報を残さない。\n\n#### リスク4: 実API統合テストの不安定化\n- **影響度**: 中\n- **確率**: 中\n- **軽減策**: 統合テストをオプトイン方式（環境変数ON時のみ実行）にし、失敗時はフォールバック確認テストを優先。\n\n## 7. 品質ゲート\n#### Phase 1: 要件定義\n- [ ] LLM統合要件と生成品質基準が明文化されている\n- [ ] セキュリティ・コスト要件が整理されている\n\n#### Phase 2: 設計\n- [ ] 実装戦略・テスト戦略・テストコード戦略の根拠が設計書に反映されている\n- [ ] フォールバックフローとエラーハンドリングが設計されている\n\n#### Phase 3: テストシナリオ\n- [ ] 成功/失敗/フォールバックを網羅したテストケースが定義されている\n- [ ] 実API統合テストの前提条件とスキップ条件が明記されている\n\n#### Phase 4: 実装\n- [ ] LLM統合コードがFeature flagで制御できる\n- [ ] フォールバック経路が手動テストで確認されている\n\n#### Phase 5: テストコード実装\n- [ ] ユニットテストでプロンプト構造とフォールバックを検証できている\n- [ ] 統合テストが環境変数制御で安全に実行できる\n\n#### Phase 6: テスト実行\n- [ ] すべてのユニットテスト/統合テストが緑である\n- [ ] 生成結果サンプルが受け入れ基準を満たしている\n\n#### Phase 7: ドキュメント\n- [ ] LLM統合の手順・設定・注意点が主要ドキュメントに記載されている\n- [ ] フォールバック時の対応手順が共有されている\n\n#### Phase 8: レポート\n- [ ] 実装差分とテスト結果がレポートに整理されている\n- [ ] 残課題と次の改善提案が記載されている\n","exit_code":0,"status":"completed"}}
{"type":"item.started","item":{"id":"item_2","type":"command_execution","command":"bash -lc 'cat .ai-workflow/issue-119/01_requirements/output/requirements.md'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_2","type":"command_execution","command":"bash -lc 'cat .ai-workflow/issue-119/01_requirements/output/requirements.md'","aggregated_output":"# 要件定義書 - Issue #119 フォローアップIssue生成品質の改善（LLM活用）\n\n## 0. Planning Documentの確認\n- `00_planning/output/planning.md` に基づき、既存 `issue-client.ts` の責務を維持しつつ新規 `issue-ai-generator.ts` でLLM連携を拡張する「EXTEND」戦略を採用する。\n- テストはモック中心のユニットテストと実APIを使用する統合テストの両輪（UNIT_INTEGRATION/BOTH_TEST）で実施し、フォールバック経路を含めて網羅する。\n- 主なリスクはLLM API失敗・品質ばらつき・機密情報漏洩であり、リトライ／フォールバック、プロンプト検証、フィルタリングで軽減する。\n- 想定工数は22〜26時間で、要件定義・設計・実装・テスト・ドキュメント更新までを段階的に進める。\n\n## 1. 概要\n既存のフォローアップIssue自動生成機能は、タイトルが一般的すぎて検索性が低く、本文も箇条書きのみで具体的な作業内容が伝わらない。また、元IssueやPRの背景情報が十分に反映されず、残タスクの意義や優先度が不明瞭である。  \n本要件はLLMを統合してタイトルと本文の生成品質を向上させ、技術的背景や実行手順を含む実務的に活用できるフォローアップIssueを自動生成することを目的とする。  \nこれにより、開発チームは手動でIssueを整形する手間を削減し、残タスクの理解・着手が迅速化され、ワークフロー全体のスループット向上と品質担保が期待できる。\n\n### 1.1 現行フォローアップIssue生成フローとデータ連携（Task 1-1）\n- **シーケンス概要**  \n  1. Evaluation Phase完了時に `RemainingTask[]` と `IssueContext` が Phase Runner から `IssueClient.createIssueFromEvaluation()` へ渡される（`src/core/github/issue-client.ts:198`）。併せて評価レポートのファイルパスが付与される。  \n  2. `createIssueFromEvaluation` 内で INFO ログを出力後、`generateFollowUpTitle` が呼び出され、最初の最大3件の残タスクを `extractKeywords` で分割・20文字以内に整形し、`[FOLLOW-UP] #<issueNumber>: <keywords>` 形式のタイトルを組み立てる（同:163行付近）。  \n  3. 本文は `IssueContext` が存在する場合に `## 背景` セクションへ `summary` を記述し、`blockerStatus` `deferredReason` をサブセクションとして展開する。`IssueContext` がない場合は定型文にフォールバックする。  \n  4. `RemainingTask` をループし、タスク番号ごとに `formatTaskDetails` が Markdown の見出し・対象ファイル一覧・作業手順・受け入れ基準・優先度・見積もり・依存タスクを整形し `## 残タスク詳細` 配下へ追加する（`src/core/github/issue-client.ts:210-274`）。  \n  5. 末尾に `## 参考` セクションを追加し、元Issue番号と `evaluationReportPath` へのリンクを差し込んだうえで `---` によるフッタを付与する。  \n  6. 完成したタイトル・本文を Octokit 経由で GitHub API `issues.create` に送信し、成功時は INFO、失敗時は `encodeWarning` 済みメッセージを ERROR ログに記録する。\n\n- **データフロー整理**  \n  | 入力ソース | 受け取り箇所 | 利用目的 |\n  | --- | --- | --- |\n  | `RemainingTask.task` | `formatTaskDetails` | `### Task n:` 見出しの文言として使用 |\n  | `RemainingTask.targetFiles[]` | `formatTaskDetails` | `**対象ファイル**` セクションにコードパスを列挙 |\n  | `RemainingTask.steps[]` | `formatTaskDetails` | 実行手順（番号付きリスト）として出力 |\n  | `RemainingTask.acceptanceCriteria[]` | `formatTaskDetails` | `- [ ]` チェックボックス付き受け入れ基準を生成 |\n  | `RemainingTask.priority` / `priorityReason` | `formatTaskDetails` | 優先度と根拠を1行にまとめて提示 |\n  | `RemainingTask.estimatedHours` | `formatTaskDetails` | 作業見積もりを `**見積もり**` として併記 |\n  | `RemainingTask.dependencies[]` | `formatTaskDetails` | 依存タスク一覧を箇条書きで記載 |\n  | `IssueContext.summary` | 本文 `## 背景` | 元Issue・PRのサマリーを冒頭に表示 |\n  | `IssueContext.blockerStatus` | 本文 `### 元 Issue のステータス` | ブロッカー解消状況を共有 |\n  | `IssueContext.deferredReason` | 本文 `### なぜこれらのタスクが残ったか` | 残タスク発生の背景説明 |\n  | `evaluationReportPath` | 本文 `## 参考` | 評価レポート位置をリンク/パスで提示 |\n\n  `RemainingTask` と `IssueContext` は `src/types.ts:36-105` に定義されており、Evaluation Phaseからの構造体がそのまま文字列化される。今後のLLM統合ではこれらの値をプロンプト入力へ拡張するが、既存フォールバック経路との互換性を維持する。\n\n## 2. 機能要件\nIssue本文に「## TODO」節は存在しないため、概要および提案解決策から抽出した要件を整理する。\n\n| ID | 要件 | 詳細 | 優先度 |\n| --- | --- | --- | --- |\n| FR-1 | LLMを用いたインテリジェントタイトル生成 | `generateIntelligentTitle` 関数で元Issue・PR・残タスクの文脈を入力に、50〜80文字の技術的に明確なタイトルを生成する。タイトルは主要コンポーネントや指標（例: 対象モジュール、目標値）を含み、既存の単語分割ロジックより情報密度が高いこと。 | 高 |\n| FR-2 | 構造化されたタスク本文生成 | `generateTaskDescription` が背景、目的、実行内容（ステップ・対象ファイル・テスト方法）、受け入れ基準、関連リソースをMarkdownセクションとして出力する。本文内で1ステップずつ実行指示を明示し、テスト手順と完了条件を含める。 | 高 |\n| FR-3 | フォールバック制御 | LLM呼び出し失敗・タイムアウト・無効化設定時には既存の `generateFollowUpTitle` / `formatTaskDetails` を自動で利用し、処理を中断させない。フォールバック発動状況は警告ログで記録する。 | 高 |\n| FR-4 | 設定オプションの拡張 | `IssueGenerationOptions` 等を通じてLLM有効化フラグ、モデル選択、タイムアウト、最大リトライ回数を指定できるようにし、CLIや環境変数から設定可能にする。デフォルト値は後方互換を保つ。 | 中 |\n| FR-5 | ログと品質監視 | LLM呼び出し成功時は入力トークン長・モデル名・生成時間をDEBUGログに出力し、失敗時は原因と再試行状況をWARNログに記録する。WARNログは `{ event_code, message, fallback_mode, retry_count }` を最低限含む構造化ペイロードとし、ログには機密情報を含めない。 | 中 |\n\n### 2.1 LLM API利用要件（Task 1-2）\n- **モデル候補とトークン制限**  \n  | モデルID | プロバイダ | 想定用途 | 最大入力トークン | 最大出力トークン | 備考 |\n  | --- | --- | --- | --- | --- | --- |\n  | `claude-3-sonnet-20240229` | Anthropic | デフォルト（高品質/安定） | 8,000 | 2,000 | コストと品質のバランスが良い。 |\n  | `gpt-4o-2024-05-13` | OpenAI | 代替（マルチリンガル/ツール連携） | 8,192 | 2,048 | 既存OpenAIインフラを活用。 |\n  | `gpt-4o-mini` | OpenAI | 低コストフォールバック | 4,096 | 1,024 | 低コスト検証やデグレ時の緊急利用。 |\n\n  - プロンプト構成は `RemainingTask` 最大5件を想定し、入力トークンが6,000を超える場合は `targetFiles`・`steps` を優先度順にトリミングし、必須フィールド（`task`, `priority`, `acceptanceCriteria`）を保持する。  \n  - レスポンスはタイトル80文字制約・本文5セクションのテンプレートに合わせ、余剰セクションがある場合はポストプロセスで削除する。\n\n- **APIパラメータとリトライ**  \n  | 項目 | 設定値 | 説明 |\n  | --- | --- | --- |\n  | `temperature` | 0.2 | 安定した出力のため低めに固定。 |\n  | `top_p` | 0.95 | 生成多様性の微調整用。 |\n  | `max_prompt_tokens` | 7,000 | 入力安全域（超過時は事前に短縮）。 |\n  | `max_completion_tokens` | 1,500 | 本文生成に十分な上限。 |\n  | `timeout_ms` | 25,000 | API呼び出しのハードタイムアウト。 |\n  | `max_retries` | 3 | HTTP503/RateLimit/Timeout時は指数バックオフ（2s, 4s, 8s）で再試行。 |\n  | `parallel_requests` | 1 | 連続生成でのレート制限回避。 |\n\n- **構成・設定項目**  \n  - 環境変数: `LLM_PROVIDER`（`anthropic`/`openai`）、`LLM_MODEL`, `LLM_TIMEOUT_MS`, `LLM_MAX_TOKENS`, `LLM_MAX_RETRIES`。未設定時は `anthropic` + `claude-3-sonnet-20240229` を採用。  \n  - CLI/設定ファイルからは `--llm-model`, `--llm-timeout`, `--llm-max-retries`, `--llm-disabled` を指定可能にし、`IssueGenerationOptions` 経由で `issue-ai-generator` へ渡す。  \n  - 成功ログは `{ event_code: 'FOLLOWUP_LLM_SUCCESS', model, input_tokens, output_tokens, duration_ms }` を、フォールバック時は `{ event_code: 'FOLLOWUP_LLM_FALLBACK', reason, retry_count, fallback_mode: 'legacy_template' }` を出力する。\n\n- **プロンプトテンプレート管理**  \n  - テンプレートは `issue-ai-generator` 内で定数として保持し、`IssueContext` の `summary` / `blockerStatus` / `deferredReason`、各 `RemainingTask` の `task`・`targetFiles`・`steps`・`acceptanceCriteria` をJSON構造で埋め込む。  \n  - プロンプトに含めるログ記録はマスク済みであることをバリデーションし、`targetFiles` を最大10件に制限。  \n  - レスポンス検証で必須セクション欠落時は `ValidationError` を発生させ、フォールバックへ移行する。\n\n## 3. 非機能要件\n- **パフォーマンス**: 単一タスクあたりのLLM呼び出しは平均15秒以内に完了し、タイムアウトは30秒以下に設定する。レート制限到達時は指数バックオフで最大3回までリトライする。\n- **セキュリティ**: APIキーは環境変数で安全に読み込み、ログ・例外メッセージに出力しない。送信ペイロードから機密情報（トークン、クレデンシャル、個人情報）は除外するフィルタリング層を備える。\n- **可用性・信頼性**: フォールバック経路はLLMが失敗しても100%動作し、呼び出し結果はエラー発生時に既存生成ロジックへ切り替える。リトライ失敗時は処理継続と警告発報を保証する。\n- **保守性・拡張性**: LLM連携は `issue-ai-generator.ts` に集約し、将来的なモデル追加やプロンプト更新を局所化する。ユニットテスト・統合テストを追加し、既存カバレッジ水準を維持または向上させる。\n\n## 4. 制約事項\n- **技術的制約**: TypeScript（Node.js 20系想定）で実装し、既存CLIアーキテクチャと互換性を保つ。LLMクライアントは既存依存（`openai`、`@anthropic-ai/claude-agent-sdk`）を利用し、新規依存追加は最小限に留める。\n- **リソース制約**: Planning成果物の見積もりに従い22〜26時間内で完了させる。開発リソースは既存チームのスプリント枠内に限定される。\n- **ポリシー制約**: CLAUDE.md/ARCHITECTURE.md に準拠し、コーディング規約・ログポリシー・セキュリティポリシーを遵守する。API利用は組織のコスト管理指針に従い、不要な試行を抑制する。\n\n## 5. 前提条件\n- **システム環境**: Node.js 20.x、TypeScript 5.x、Jestベースのテスト環境、GitHub APIアクセス権限、LLM API（ClaudeまたはCodex/OpenAI）へのネットワーク接続。\n- **依存コンポーネント**: 既存 `issue-client.ts`、`commands/execute/agent-setup.ts`、GitHub Issue作成フロー、`openai` / `@anthropic-ai/claude-agent-sdk` クライアント。\n- **外部システム連携**: GitHub REST API、Claude/OpenAI API。APIキーは環境変数（例: `ANTHROPIC_API_KEY`, `OPENAI_API_KEY`）で提供される。\n\n## 6. 受け入れ基準\n- FR-1  \n  - Given LLMが有効で必要な認証情報が設定されている  \n  - When `generateIntelligentTitle` にIssue・PR・タスク文脈を渡して実行する  \n  - Then 50〜80文字のタイトルが生成され、主要技術要素が含まれ、空文字や重複語が発生しない\n- FR-2  \n  - Given 残タスク情報とIssue/PRコンテキストが揃っている  \n  - When `generateTaskDescription` を呼び出す  \n  - Then 背景・目的・実行内容・受け入れ基準・関連リソースのMarkdownセクションがすべて出力され、実行内容にステップとテスト方法が含まれる\n- FR-3  \n  - Given LLM呼び出しがタイムアウトまたはエラーを返す  \n  - When フォローアップIssue生成を行う  \n  - Then 既存ロジックでタイトルと本文が生成され、WARNログにフォールバック理由が記録される\n- FR-4  \n  - Given CLI実行時にLLM関連オプションを指定する  \n  - When フォローアップIssue生成を実行する  \n  - Then 指定されたモデル・タイムアウト・リトライ設定が `issue-ai-generator` に反映され、設定が無効な場合はバリデーションエラーを返す\n- FR-5  \n  - Given LLM呼び出しが行われ、成功または失敗する  \n  - When ログ出力を確認する  \n  - Then 成功時にモデル名・処理時間等がDEBUGで、失敗時に原因と再試行情報がWARNで出力され、ログに機密情報が含まれない\n\n## 7. スコープ外\n- カスタムプロンプトをユーザーが任意に設定する機能（`.ai-workflow/config.yml` 拡張）は今回のスコープ外で将来検討とする。\n- 生成されたIssue内容の人手レビュー・承認ワークフロー自動化は対象外。\n- 英語以外の多言語サポートやプロジェクト固有テンプレートの自動切替は本フェーズでは扱わない。\n","exit_code":0,"status":"completed"}}
{"type":"item.started","item":{"id":"item_3","type":"command_execution","command":"bash -lc \"sed -n '125,250p' .ai-workflow/issue-119/01_requirements/output/requirements.md\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_3","type":"command_execution","command":"bash -lc \"sed -n '125,250p' .ai-workflow/issue-119/01_requirements/output/requirements.md\"","aggregated_output":"","exit_code":0,"status":"completed"}}
{"type":"item.started","item":{"id":"item_4","type":"command_execution","command":"bash -lc 'wc -l .ai-workflow/issue-119/01_requirements/output/requirements.md'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_4","type":"command_execution","command":"bash -lc 'wc -l .ai-workflow/issue-119/01_requirements/output/requirements.md'","aggregated_output":"124 .ai-workflow/issue-119/01_requirements/output/requirements.md\n","exit_code":0,"status":"completed"}}
{"type":"item.started","item":{"id":"item_5","type":"command_execution","command":"bash -lc \"sed -n '1,124p' .ai-workflow/issue-119/01_requirements/output/requirements.md\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_5","type":"command_execution","command":"bash -lc \"sed -n '1,124p' .ai-workflow/issue-119/01_requirements/output/requirements.md\"","aggregated_output":"# 要件定義書 - Issue #119 フォローアップIssue生成品質の改善（LLM活用）\n\n## 0. Planning Documentの確認\n- `00_planning/output/planning.md` に基づき、既存 `issue-client.ts` の責務を維持しつつ新規 `issue-ai-generator.ts` でLLM連携を拡張する「EXTEND」戦略を採用する。\n- テストはモック中心のユニットテストと実APIを使用する統合テストの両輪（UNIT_INTEGRATION/BOTH_TEST）で実施し、フォールバック経路を含めて網羅する。\n- 主なリスクはLLM API失敗・品質ばらつき・機密情報漏洩であり、リトライ／フォールバック、プロンプト検証、フィルタリングで軽減する。\n- 想定工数は22〜26時間で、要件定義・設計・実装・テスト・ドキュメント更新までを段階的に進める。\n\n## 1. 概要\n既存のフォローアップIssue自動生成機能は、タイトルが一般的すぎて検索性が低く、本文も箇条書きのみで具体的な作業内容が伝わらない。また、元IssueやPRの背景情報が十分に反映されず、残タスクの意義や優先度が不明瞭である。  \n本要件はLLMを統合してタイトルと本文の生成品質を向上させ、技術的背景や実行手順を含む実務的に活用できるフォローアップIssueを自動生成することを目的とする。  \nこれにより、開発チームは手動でIssueを整形する手間を削減し、残タスクの理解・着手が迅速化され、ワークフロー全体のスループット向上と品質担保が期待できる。\n\n### 1.1 現行フォローアップIssue生成フローとデータ連携（Task 1-1）\n- **シーケンス概要**  \n  1. Evaluation Phase完了時に `RemainingTask[]` と `IssueContext` が Phase Runner から `IssueClient.createIssueFromEvaluation()` へ渡される（`src/core/github/issue-client.ts:198`）。併せて評価レポートのファイルパスが付与される。  \n  2. `createIssueFromEvaluation` 内で INFO ログを出力後、`generateFollowUpTitle` が呼び出され、最初の最大3件の残タスクを `extractKeywords` で分割・20文字以内に整形し、`[FOLLOW-UP] #<issueNumber>: <keywords>` 形式のタイトルを組み立てる（同:163行付近）。  \n  3. 本文は `IssueContext` が存在する場合に `## 背景` セクションへ `summary` を記述し、`blockerStatus` `deferredReason` をサブセクションとして展開する。`IssueContext` がない場合は定型文にフォールバックする。  \n  4. `RemainingTask` をループし、タスク番号ごとに `formatTaskDetails` が Markdown の見出し・対象ファイル一覧・作業手順・受け入れ基準・優先度・見積もり・依存タスクを整形し `## 残タスク詳細` 配下へ追加する（`src/core/github/issue-client.ts:210-274`）。  \n  5. 末尾に `## 参考` セクションを追加し、元Issue番号と `evaluationReportPath` へのリンクを差し込んだうえで `---` によるフッタを付与する。  \n  6. 完成したタイトル・本文を Octokit 経由で GitHub API `issues.create` に送信し、成功時は INFO、失敗時は `encodeWarning` 済みメッセージを ERROR ログに記録する。\n\n- **データフロー整理**  \n  | 入力ソース | 受け取り箇所 | 利用目的 |\n  | --- | --- | --- |\n  | `RemainingTask.task` | `formatTaskDetails` | `### Task n:` 見出しの文言として使用 |\n  | `RemainingTask.targetFiles[]` | `formatTaskDetails` | `**対象ファイル**` セクションにコードパスを列挙 |\n  | `RemainingTask.steps[]` | `formatTaskDetails` | 実行手順（番号付きリスト）として出力 |\n  | `RemainingTask.acceptanceCriteria[]` | `formatTaskDetails` | `- [ ]` チェックボックス付き受け入れ基準を生成 |\n  | `RemainingTask.priority` / `priorityReason` | `formatTaskDetails` | 優先度と根拠を1行にまとめて提示 |\n  | `RemainingTask.estimatedHours` | `formatTaskDetails` | 作業見積もりを `**見積もり**` として併記 |\n  | `RemainingTask.dependencies[]` | `formatTaskDetails` | 依存タスク一覧を箇条書きで記載 |\n  | `IssueContext.summary` | 本文 `## 背景` | 元Issue・PRのサマリーを冒頭に表示 |\n  | `IssueContext.blockerStatus` | 本文 `### 元 Issue のステータス` | ブロッカー解消状況を共有 |\n  | `IssueContext.deferredReason` | 本文 `### なぜこれらのタスクが残ったか` | 残タスク発生の背景説明 |\n  | `evaluationReportPath` | 本文 `## 参考` | 評価レポート位置をリンク/パスで提示 |\n\n  `RemainingTask` と `IssueContext` は `src/types.ts:36-105` に定義されており、Evaluation Phaseからの構造体がそのまま文字列化される。今後のLLM統合ではこれらの値をプロンプト入力へ拡張するが、既存フォールバック経路との互換性を維持する。\n\n## 2. 機能要件\nIssue本文に「## TODO」節は存在しないため、概要および提案解決策から抽出した要件を整理する。\n\n| ID | 要件 | 詳細 | 優先度 |\n| --- | --- | --- | --- |\n| FR-1 | LLMを用いたインテリジェントタイトル生成 | `generateIntelligentTitle` 関数で元Issue・PR・残タスクの文脈を入力に、50〜80文字の技術的に明確なタイトルを生成する。タイトルは主要コンポーネントや指標（例: 対象モジュール、目標値）を含み、既存の単語分割ロジックより情報密度が高いこと。 | 高 |\n| FR-2 | 構造化されたタスク本文生成 | `generateTaskDescription` が背景、目的、実行内容（ステップ・対象ファイル・テスト方法）、受け入れ基準、関連リソースをMarkdownセクションとして出力する。本文内で1ステップずつ実行指示を明示し、テスト手順と完了条件を含める。 | 高 |\n| FR-3 | フォールバック制御 | LLM呼び出し失敗・タイムアウト・無効化設定時には既存の `generateFollowUpTitle` / `formatTaskDetails` を自動で利用し、処理を中断させない。フォールバック発動状況は警告ログで記録する。 | 高 |\n| FR-4 | 設定オプションの拡張 | `IssueGenerationOptions` 等を通じてLLM有効化フラグ、モデル選択、タイムアウト、最大リトライ回数を指定できるようにし、CLIや環境変数から設定可能にする。デフォルト値は後方互換を保つ。 | 中 |\n| FR-5 | ログと品質監視 | LLM呼び出し成功時は入力トークン長・モデル名・生成時間をDEBUGログに出力し、失敗時は原因と再試行状況をWARNログに記録する。WARNログは `{ event_code, message, fallback_mode, retry_count }` を最低限含む構造化ペイロードとし、ログには機密情報を含めない。 | 中 |\n\n### 2.1 LLM API利用要件（Task 1-2）\n- **モデル候補とトークン制限**  \n  | モデルID | プロバイダ | 想定用途 | 最大入力トークン | 最大出力トークン | 備考 |\n  | --- | --- | --- | --- | --- | --- |\n  | `claude-3-sonnet-20240229` | Anthropic | デフォルト（高品質/安定） | 8,000 | 2,000 | コストと品質のバランスが良い。 |\n  | `gpt-4o-2024-05-13` | OpenAI | 代替（マルチリンガル/ツール連携） | 8,192 | 2,048 | 既存OpenAIインフラを活用。 |\n  | `gpt-4o-mini` | OpenAI | 低コストフォールバック | 4,096 | 1,024 | 低コスト検証やデグレ時の緊急利用。 |\n\n  - プロンプト構成は `RemainingTask` 最大5件を想定し、入力トークンが6,000を超える場合は `targetFiles`・`steps` を優先度順にトリミングし、必須フィールド（`task`, `priority`, `acceptanceCriteria`）を保持する。  \n  - レスポンスはタイトル80文字制約・本文5セクションのテンプレートに合わせ、余剰セクションがある場合はポストプロセスで削除する。\n\n- **APIパラメータとリトライ**  \n  | 項目 | 設定値 | 説明 |\n  | --- | --- | --- |\n  | `temperature` | 0.2 | 安定した出力のため低めに固定。 |\n  | `top_p` | 0.95 | 生成多様性の微調整用。 |\n  | `max_prompt_tokens` | 7,000 | 入力安全域（超過時は事前に短縮）。 |\n  | `max_completion_tokens` | 1,500 | 本文生成に十分な上限。 |\n  | `timeout_ms` | 25,000 | API呼び出しのハードタイムアウト。 |\n  | `max_retries` | 3 | HTTP503/RateLimit/Timeout時は指数バックオフ（2s, 4s, 8s）で再試行。 |\n  | `parallel_requests` | 1 | 連続生成でのレート制限回避。 |\n\n- **構成・設定項目**  \n  - 環境変数: `LLM_PROVIDER`（`anthropic`/`openai`）、`LLM_MODEL`, `LLM_TIMEOUT_MS`, `LLM_MAX_TOKENS`, `LLM_MAX_RETRIES`。未設定時は `anthropic` + `claude-3-sonnet-20240229` を採用。  \n  - CLI/設定ファイルからは `--llm-model`, `--llm-timeout`, `--llm-max-retries`, `--llm-disabled` を指定可能にし、`IssueGenerationOptions` 経由で `issue-ai-generator` へ渡す。  \n  - 成功ログは `{ event_code: 'FOLLOWUP_LLM_SUCCESS', model, input_tokens, output_tokens, duration_ms }` を、フォールバック時は `{ event_code: 'FOLLOWUP_LLM_FALLBACK', reason, retry_count, fallback_mode: 'legacy_template' }` を出力する。\n\n- **プロンプトテンプレート管理**  \n  - テンプレートは `issue-ai-generator` 内で定数として保持し、`IssueContext` の `summary` / `blockerStatus` / `deferredReason`、各 `RemainingTask` の `task`・`targetFiles`・`steps`・`acceptanceCriteria` をJSON構造で埋め込む。  \n  - プロンプトに含めるログ記録はマスク済みであることをバリデーションし、`targetFiles` を最大10件に制限。  \n  - レスポンス検証で必須セクション欠落時は `ValidationError` を発生させ、フォールバックへ移行する。\n\n## 3. 非機能要件\n- **パフォーマンス**: 単一タスクあたりのLLM呼び出しは平均15秒以内に完了し、タイムアウトは30秒以下に設定する。レート制限到達時は指数バックオフで最大3回までリトライする。\n- **セキュリティ**: APIキーは環境変数で安全に読み込み、ログ・例外メッセージに出力しない。送信ペイロードから機密情報（トークン、クレデンシャル、個人情報）は除外するフィルタリング層を備える。\n- **可用性・信頼性**: フォールバック経路はLLMが失敗しても100%動作し、呼び出し結果はエラー発生時に既存生成ロジックへ切り替える。リトライ失敗時は処理継続と警告発報を保証する。\n- **保守性・拡張性**: LLM連携は `issue-ai-generator.ts` に集約し、将来的なモデル追加やプロンプト更新を局所化する。ユニットテスト・統合テストを追加し、既存カバレッジ水準を維持または向上させる。\n\n## 4. 制約事項\n- **技術的制約**: TypeScript（Node.js 20系想定）で実装し、既存CLIアーキテクチャと互換性を保つ。LLMクライアントは既存依存（`openai`、`@anthropic-ai/claude-agent-sdk`）を利用し、新規依存追加は最小限に留める。\n- **リソース制約**: Planning成果物の見積もりに従い22〜26時間内で完了させる。開発リソースは既存チームのスプリント枠内に限定される。\n- **ポリシー制約**: CLAUDE.md/ARCHITECTURE.md に準拠し、コーディング規約・ログポリシー・セキュリティポリシーを遵守する。API利用は組織のコスト管理指針に従い、不要な試行を抑制する。\n\n## 5. 前提条件\n- **システム環境**: Node.js 20.x、TypeScript 5.x、Jestベースのテスト環境、GitHub APIアクセス権限、LLM API（ClaudeまたはCodex/OpenAI）へのネットワーク接続。\n- **依存コンポーネント**: 既存 `issue-client.ts`、`commands/execute/agent-setup.ts`、GitHub Issue作成フロー、`openai` / `@anthropic-ai/claude-agent-sdk` クライアント。\n- **外部システム連携**: GitHub REST API、Claude/OpenAI API。APIキーは環境変数（例: `ANTHROPIC_API_KEY`, `OPENAI_API_KEY`）で提供される。\n\n## 6. 受け入れ基準\n- FR-1  \n  - Given LLMが有効で必要な認証情報が設定されている  \n  - When `generateIntelligentTitle` にIssue・PR・タスク文脈を渡して実行する  \n  - Then 50〜80文字のタイトルが生成され、主要技術要素が含まれ、空文字や重複語が発生しない\n- FR-2  \n  - Given 残タスク情報とIssue/PRコンテキストが揃っている  \n  - When `generateTaskDescription` を呼び出す  \n  - Then 背景・目的・実行内容・受け入れ基準・関連リソースのMarkdownセクションがすべて出力され、実行内容にステップとテスト方法が含まれる\n- FR-3  \n  - Given LLM呼び出しがタイムアウトまたはエラーを返す  \n  - When フォローアップIssue生成を行う  \n  - Then 既存ロジックでタイトルと本文が生成され、WARNログにフォールバック理由が記録される\n- FR-4  \n  - Given CLI実行時にLLM関連オプションを指定する  \n  - When フォローアップIssue生成を実行する  \n  - Then 指定されたモデル・タイムアウト・リトライ設定が `issue-ai-generator` に反映され、設定が無効な場合はバリデーションエラーを返す\n- FR-5  \n  - Given LLM呼び出しが行われ、成功または失敗する  \n  - When ログ出力を確認する  \n  - Then 成功時にモデル名・処理時間等がDEBUGで、失敗時に原因と再試行情報がWARNで出力され、ログに機密情報が含まれない\n\n## 7. スコープ外\n- カスタムプロンプトをユーザーが任意に設定する機能（`.ai-workflow/config.yml` 拡張）は今回のスコープ外で将来検討とする。\n- 生成されたIssue内容の人手レビュー・承認ワークフロー自動化は対象外。\n- 英語以外の多言語サポートやプロジェクト固有テンプレートの自動切替は本フェーズでは扱わない。\n","exit_code":0,"status":"completed"}}
{"type":"item.started","item":{"id":"item_6","type":"command_execution","command":"bash -lc 'tail -n 100 .ai-workflow/issue-119/01_requirements/output/requirements.md'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_6","type":"command_execution","command":"bash -lc 'tail -n 100 .ai-workflow/issue-119/01_requirements/output/requirements.md'","aggregated_output":"  | --- | --- | --- |\n  | `RemainingTask.task` | `formatTaskDetails` | `### Task n:` 見出しの文言として使用 |\n  | `RemainingTask.targetFiles[]` | `formatTaskDetails` | `**対象ファイル**` セクションにコードパスを列挙 |\n  | `RemainingTask.steps[]` | `formatTaskDetails` | 実行手順（番号付きリスト）として出力 |\n  | `RemainingTask.acceptanceCriteria[]` | `formatTaskDetails` | `- [ ]` チェックボックス付き受け入れ基準を生成 |\n  | `RemainingTask.priority` / `priorityReason` | `formatTaskDetails` | 優先度と根拠を1行にまとめて提示 |\n  | `RemainingTask.estimatedHours` | `formatTaskDetails` | 作業見積もりを `**見積もり**` として併記 |\n  | `RemainingTask.dependencies[]` | `formatTaskDetails` | 依存タスク一覧を箇条書きで記載 |\n  | `IssueContext.summary` | 本文 `## 背景` | 元Issue・PRのサマリーを冒頭に表示 |\n  | `IssueContext.blockerStatus` | 本文 `### 元 Issue のステータス` | ブロッカー解消状況を共有 |\n  | `IssueContext.deferredReason` | 本文 `### なぜこれらのタスクが残ったか` | 残タスク発生の背景説明 |\n  | `evaluationReportPath` | 本文 `## 参考` | 評価レポート位置をリンク/パスで提示 |\n\n  `RemainingTask` と `IssueContext` は `src/types.ts:36-105` に定義されており、Evaluation Phaseからの構造体がそのまま文字列化される。今後のLLM統合ではこれらの値をプロンプト入力へ拡張するが、既存フォールバック経路との互換性を維持する。\n\n## 2. 機能要件\nIssue本文に「## TODO」節は存在しないため、概要および提案解決策から抽出した要件を整理する。\n\n| ID | 要件 | 詳細 | 優先度 |\n| --- | --- | --- | --- |\n| FR-1 | LLMを用いたインテリジェントタイトル生成 | `generateIntelligentTitle` 関数で元Issue・PR・残タスクの文脈を入力に、50〜80文字の技術的に明確なタイトルを生成する。タイトルは主要コンポーネントや指標（例: 対象モジュール、目標値）を含み、既存の単語分割ロジックより情報密度が高いこと。 | 高 |\n| FR-2 | 構造化されたタスク本文生成 | `generateTaskDescription` が背景、目的、実行内容（ステップ・対象ファイル・テスト方法）、受け入れ基準、関連リソースをMarkdownセクションとして出力する。本文内で1ステップずつ実行指示を明示し、テスト手順と完了条件を含める。 | 高 |\n| FR-3 | フォールバック制御 | LLM呼び出し失敗・タイムアウト・無効化設定時には既存の `generateFollowUpTitle` / `formatTaskDetails` を自動で利用し、処理を中断させない。フォールバック発動状況は警告ログで記録する。 | 高 |\n| FR-4 | 設定オプションの拡張 | `IssueGenerationOptions` 等を通じてLLM有効化フラグ、モデル選択、タイムアウト、最大リトライ回数を指定できるようにし、CLIや環境変数から設定可能にする。デフォルト値は後方互換を保つ。 | 中 |\n| FR-5 | ログと品質監視 | LLM呼び出し成功時は入力トークン長・モデル名・生成時間をDEBUGログに出力し、失敗時は原因と再試行状況をWARNログに記録する。WARNログは `{ event_code, message, fallback_mode, retry_count }` を最低限含む構造化ペイロードとし、ログには機密情報を含めない。 | 中 |\n\n### 2.1 LLM API利用要件（Task 1-2）\n- **モデル候補とトークン制限**  \n  | モデルID | プロバイダ | 想定用途 | 最大入力トークン | 最大出力トークン | 備考 |\n  | --- | --- | --- | --- | --- | --- |\n  | `claude-3-sonnet-20240229` | Anthropic | デフォルト（高品質/安定） | 8,000 | 2,000 | コストと品質のバランスが良い。 |\n  | `gpt-4o-2024-05-13` | OpenAI | 代替（マルチリンガル/ツール連携） | 8,192 | 2,048 | 既存OpenAIインフラを活用。 |\n  | `gpt-4o-mini` | OpenAI | 低コストフォールバック | 4,096 | 1,024 | 低コスト検証やデグレ時の緊急利用。 |\n\n  - プロンプト構成は `RemainingTask` 最大5件を想定し、入力トークンが6,000を超える場合は `targetFiles`・`steps` を優先度順にトリミングし、必須フィールド（`task`, `priority`, `acceptanceCriteria`）を保持する。  \n  - レスポンスはタイトル80文字制約・本文5セクションのテンプレートに合わせ、余剰セクションがある場合はポストプロセスで削除する。\n\n- **APIパラメータとリトライ**  \n  | 項目 | 設定値 | 説明 |\n  | --- | --- | --- |\n  | `temperature` | 0.2 | 安定した出力のため低めに固定。 |\n  | `top_p` | 0.95 | 生成多様性の微調整用。 |\n  | `max_prompt_tokens` | 7,000 | 入力安全域（超過時は事前に短縮）。 |\n  | `max_completion_tokens` | 1,500 | 本文生成に十分な上限。 |\n  | `timeout_ms` | 25,000 | API呼び出しのハードタイムアウト。 |\n  | `max_retries` | 3 | HTTP503/RateLimit/Timeout時は指数バックオフ（2s, 4s, 8s）で再試行。 |\n  | `parallel_requests` | 1 | 連続生成でのレート制限回避。 |\n\n- **構成・設定項目**  \n  - 環境変数: `LLM_PROVIDER`（`anthropic`/`openai`）、`LLM_MODEL`, `LLM_TIMEOUT_MS`, `LLM_MAX_TOKENS`, `LLM_MAX_RETRIES`。未設定時は `anthropic` + `claude-3-sonnet-20240229` を採用。  \n  - CLI/設定ファイルからは `--llm-model`, `--llm-timeout`, `--llm-max-retries`, `--llm-disabled` を指定可能にし、`IssueGenerationOptions` 経由で `issue-ai-generator` へ渡す。  \n  - 成功ログは `{ event_code: 'FOLLOWUP_LLM_SUCCESS', model, input_tokens, output_tokens, duration_ms }` を、フォールバック時は `{ event_code: 'FOLLOWUP_LLM_FALLBACK', reason, retry_count, fallback_mode: 'legacy_template' }` を出力する。\n\n- **プロンプトテンプレート管理**  \n  - テンプレートは `issue-ai-generator` 内で定数として保持し、`IssueContext` の `summary` / `blockerStatus` / `deferredReason`、各 `RemainingTask` の `task`・`targetFiles`・`steps`・`acceptanceCriteria` をJSON構造で埋め込む。  \n  - プロンプトに含めるログ記録はマスク済みであることをバリデーションし、`targetFiles` を最大10件に制限。  \n  - レスポンス検証で必須セクション欠落時は `ValidationError` を発生させ、フォールバックへ移行する。\n\n## 3. 非機能要件\n- **パフォーマンス**: 単一タスクあたりのLLM呼び出しは平均15秒以内に完了し、タイムアウトは30秒以下に設定する。レート制限到達時は指数バックオフで最大3回までリトライする。\n- **セキュリティ**: APIキーは環境変数で安全に読み込み、ログ・例外メッセージに出力しない。送信ペイロードから機密情報（トークン、クレデンシャル、個人情報）は除外するフィルタリング層を備える。\n- **可用性・信頼性**: フォールバック経路はLLMが失敗しても100%動作し、呼び出し結果はエラー発生時に既存生成ロジックへ切り替える。リトライ失敗時は処理継続と警告発報を保証する。\n- **保守性・拡張性**: LLM連携は `issue-ai-generator.ts` に集約し、将来的なモデル追加やプロンプト更新を局所化する。ユニットテスト・統合テストを追加し、既存カバレッジ水準を維持または向上させる。\n\n## 4. 制約事項\n- **技術的制約**: TypeScript（Node.js 20系想定）で実装し、既存CLIアーキテクチャと互換性を保つ。LLMクライアントは既存依存（`openai`、`@anthropic-ai/claude-agent-sdk`）を利用し、新規依存追加は最小限に留める。\n- **リソース制約**: Planning成果物の見積もりに従い22〜26時間内で完了させる。開発リソースは既存チームのスプリント枠内に限定される。\n- **ポリシー制約**: CLAUDE.md/ARCHITECTURE.md に準拠し、コーディング規約・ログポリシー・セキュリティポリシーを遵守する。API利用は組織のコスト管理指針に従い、不要な試行を抑制する。\n\n## 5. 前提条件\n- **システム環境**: Node.js 20.x、TypeScript 5.x、Jestベースのテスト環境、GitHub APIアクセス権限、LLM API（ClaudeまたはCodex/OpenAI）へのネットワーク接続。\n- **依存コンポーネント**: 既存 `issue-client.ts`、`commands/execute/agent-setup.ts`、GitHub Issue作成フロー、`openai` / `@anthropic-ai/claude-agent-sdk` クライアント。\n- **外部システム連携**: GitHub REST API、Claude/OpenAI API。APIキーは環境変数（例: `ANTHROPIC_API_KEY`, `OPENAI_API_KEY`）で提供される。\n\n## 6. 受け入れ基準\n- FR-1  \n  - Given LLMが有効で必要な認証情報が設定されている  \n  - When `generateIntelligentTitle` にIssue・PR・タスク文脈を渡して実行する  \n  - Then 50〜80文字のタイトルが生成され、主要技術要素が含まれ、空文字や重複語が発生しない\n- FR-2  \n  - Given 残タスク情報とIssue/PRコンテキストが揃っている  \n  - When `generateTaskDescription` を呼び出す  \n  - Then 背景・目的・実行内容・受け入れ基準・関連リソースのMarkdownセクションがすべて出力され、実行内容にステップとテスト方法が含まれる\n- FR-3  \n  - Given LLM呼び出しがタイムアウトまたはエラーを返す  \n  - When フォローアップIssue生成を行う  \n  - Then 既存ロジックでタイトルと本文が生成され、WARNログにフォールバック理由が記録される\n- FR-4  \n  - Given CLI実行時にLLM関連オプションを指定する  \n  - When フォローアップIssue生成を実行する  \n  - Then 指定されたモデル・タイムアウト・リトライ設定が `issue-ai-generator` に反映され、設定が無効な場合はバリデーションエラーを返す\n- FR-5  \n  - Given LLM呼び出しが行われ、成功または失敗する  \n  - When ログ出力を確認する  \n  - Then 成功時にモデル名・処理時間等がDEBUGで、失敗時に原因と再試行情報がWARNで出力され、ログに機密情報が含まれない\n\n## 7. スコープ外\n- カスタムプロンプトをユーザーが任意に設定する機能（`.ai-workflow/config.yml` 拡張）は今回のスコープ外で将来検討とする。\n- 生成されたIssue内容の人手レビュー・承認ワークフロー自動化は対象外。\n- 英語以外の多言語サポートやプロジェクト固有テンプレートの自動切替は本フェーズでは扱わない。\n","exit_code":0,"status":"completed"}}
{"type":"item.started","item":{"id":"item_7","type":"command_execution","command":"bash -lc \"sed -n '60,120p' .ai-workflow/issue-119/01_requirements/output/requirements.md\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_7","type":"command_execution","command":"bash -lc \"sed -n '60,120p' .ai-workflow/issue-119/01_requirements/output/requirements.md\"","aggregated_output":"  - レスポンスはタイトル80文字制約・本文5セクションのテンプレートに合わせ、余剰セクションがある場合はポストプロセスで削除する。\n\n- **APIパラメータとリトライ**  \n  | 項目 | 設定値 | 説明 |\n  | --- | --- | --- |\n  | `temperature` | 0.2 | 安定した出力のため低めに固定。 |\n  | `top_p` | 0.95 | 生成多様性の微調整用。 |\n  | `max_prompt_tokens` | 7,000 | 入力安全域（超過時は事前に短縮）。 |\n  | `max_completion_tokens` | 1,500 | 本文生成に十分な上限。 |\n  | `timeout_ms` | 25,000 | API呼び出しのハードタイムアウト。 |\n  | `max_retries` | 3 | HTTP503/RateLimit/Timeout時は指数バックオフ（2s, 4s, 8s）で再試行。 |\n  | `parallel_requests` | 1 | 連続生成でのレート制限回避。 |\n\n- **構成・設定項目**  \n  - 環境変数: `LLM_PROVIDER`（`anthropic`/`openai`）、`LLM_MODEL`, `LLM_TIMEOUT_MS`, `LLM_MAX_TOKENS`, `LLM_MAX_RETRIES`。未設定時は `anthropic` + `claude-3-sonnet-20240229` を採用。  \n  - CLI/設定ファイルからは `--llm-model`, `--llm-timeout`, `--llm-max-retries`, `--llm-disabled` を指定可能にし、`IssueGenerationOptions` 経由で `issue-ai-generator` へ渡す。  \n  - 成功ログは `{ event_code: 'FOLLOWUP_LLM_SUCCESS', model, input_tokens, output_tokens, duration_ms }` を、フォールバック時は `{ event_code: 'FOLLOWUP_LLM_FALLBACK', reason, retry_count, fallback_mode: 'legacy_template' }` を出力する。\n\n- **プロンプトテンプレート管理**  \n  - テンプレートは `issue-ai-generator` 内で定数として保持し、`IssueContext` の `summary` / `blockerStatus` / `deferredReason`、各 `RemainingTask` の `task`・`targetFiles`・`steps`・`acceptanceCriteria` をJSON構造で埋め込む。  \n  - プロンプトに含めるログ記録はマスク済みであることをバリデーションし、`targetFiles` を最大10件に制限。  \n  - レスポンス検証で必須セクション欠落時は `ValidationError` を発生させ、フォールバックへ移行する。\n\n## 3. 非機能要件\n- **パフォーマンス**: 単一タスクあたりのLLM呼び出しは平均15秒以内に完了し、タイムアウトは30秒以下に設定する。レート制限到達時は指数バックオフで最大3回までリトライする。\n- **セキュリティ**: APIキーは環境変数で安全に読み込み、ログ・例外メッセージに出力しない。送信ペイロードから機密情報（トークン、クレデンシャル、個人情報）は除外するフィルタリング層を備える。\n- **可用性・信頼性**: フォールバック経路はLLMが失敗しても100%動作し、呼び出し結果はエラー発生時に既存生成ロジックへ切り替える。リトライ失敗時は処理継続と警告発報を保証する。\n- **保守性・拡張性**: LLM連携は `issue-ai-generator.ts` に集約し、将来的なモデル追加やプロンプト更新を局所化する。ユニットテスト・統合テストを追加し、既存カバレッジ水準を維持または向上させる。\n\n## 4. 制約事項\n- **技術的制約**: TypeScript（Node.js 20系想定）で実装し、既存CLIアーキテクチャと互換性を保つ。LLMクライアントは既存依存（`openai`、`@anthropic-ai/claude-agent-sdk`）を利用し、新規依存追加は最小限に留める。\n- **リソース制約**: Planning成果物の見積もりに従い22〜26時間内で完了させる。開発リソースは既存チームのスプリント枠内に限定される。\n- **ポリシー制約**: CLAUDE.md/ARCHITECTURE.md に準拠し、コーディング規約・ログポリシー・セキュリティポリシーを遵守する。API利用は組織のコスト管理指針に従い、不要な試行を抑制する。\n\n## 5. 前提条件\n- **システム環境**: Node.js 20.x、TypeScript 5.x、Jestベースのテスト環境、GitHub APIアクセス権限、LLM API（ClaudeまたはCodex/OpenAI）へのネットワーク接続。\n- **依存コンポーネント**: 既存 `issue-client.ts`、`commands/execute/agent-setup.ts`、GitHub Issue作成フロー、`openai` / `@anthropic-ai/claude-agent-sdk` クライアント。\n- **外部システム連携**: GitHub REST API、Claude/OpenAI API。APIキーは環境変数（例: `ANTHROPIC_API_KEY`, `OPENAI_API_KEY`）で提供される。\n\n## 6. 受け入れ基準\n- FR-1  \n  - Given LLMが有効で必要な認証情報が設定されている  \n  - When `generateIntelligentTitle` にIssue・PR・タスク文脈を渡して実行する  \n  - Then 50〜80文字のタイトルが生成され、主要技術要素が含まれ、空文字や重複語が発生しない\n- FR-2  \n  - Given 残タスク情報とIssue/PRコンテキストが揃っている  \n  - When `generateTaskDescription` を呼び出す  \n  - Then 背景・目的・実行内容・受け入れ基準・関連リソースのMarkdownセクションがすべて出力され、実行内容にステップとテスト方法が含まれる\n- FR-3  \n  - Given LLM呼び出しがタイムアウトまたはエラーを返す  \n  - When フォローアップIssue生成を行う  \n  - Then 既存ロジックでタイトルと本文が生成され、WARNログにフォールバック理由が記録される\n- FR-4  \n  - Given CLI実行時にLLM関連オプションを指定する  \n  - When フォローアップIssue生成を実行する  \n  - Then 指定されたモデル・タイムアウト・リトライ設定が `issue-ai-generator` に反映され、設定が無効な場合はバリデーションエラーを返す\n- FR-5  \n  - Given LLM呼び出しが行われ、成功または失敗する  \n  - When ログ出力を確認する  \n  - Then 成功時にモデル名・処理時間等がDEBUGで、失敗時に原因と再試行情報がWARNで出力され、ログに機密情報が含まれない\n\n","exit_code":0,"status":"completed"}}
{"type":"item.started","item":{"id":"item_8","type":"command_execution","command":"bash -lc \"sed -n '120,180p' .ai-workflow/issue-119/01_requirements/output/requirements.md\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_8","type":"command_execution","command":"bash -lc \"sed -n '120,180p' .ai-workflow/issue-119/01_requirements/output/requirements.md\"","aggregated_output":"\n## 7. スコープ外\n- カスタムプロンプトをユーザーが任意に設定する機能（`.ai-workflow/config.yml` 拡張）は今回のスコープ外で将来検討とする。\n- 生成されたIssue内容の人手レビュー・承認ワークフロー自動化は対象外。\n- 英語以外の多言語サポートやプロジェクト固有テンプレートの自動切替は本フェーズでは扱わない。\n","exit_code":0,"status":"completed"}}
{"type":"item.started","item":{"id":"item_9","type":"command_execution","command":"bash -lc 'cat .ai-workflow/issue-119/02_design/output/design.md'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_9","type":"command_execution","command":"bash -lc 'cat .ai-workflow/issue-119/02_design/output/design.md'","aggregated_output":"# 詳細設計書 - Issue #119 フォローアップIssue生成品質の改善（LLM活用）\n\n**Issue番号**: #119  \n**タイトル**: フォローアップIssue生成品質の改善（LLM活用）  \n**バージョン**: 1.1 (Design)\n\n---\n\n## 1. アーキテクチャ設計\n\n### 1.1 システム全体フロー\n\n```\nEvaluationPhase (Phase 9)\n    │ RemainingTask[], IssueContext, evaluation report path, generation options\n    ▼\nGitHubClient.createIssueFromEvaluation(...)\n    │ delegates\n    ▼\nIssueClient (LLM-aware)\n    ├─ IssueAIGenerator.generate(...)\n    │     └─ LLM Provider Adapter (OpenAI / Anthropic)\n    └─ Legacy builders (generateFollowUpTitle + buildLegacyBody)\n    ▼\nOctokit.issues.create(...) → GitHub Issue\n```\n\n### 1.2 コンポーネント責務\n\n| コンポーネント | 役割 | 主な入出力 |\n| --- | --- | --- |\n| `EvaluationPhase` | Phase 9 の処理。残タスク検出後に GitHubClient へフォローアップ生成を依頼 | 入力: Evaluation結果 / 出力: IssueGenerationOptions 付き呼び出し |\n| `GitHubClient` | GitHub API ファサード。IssueClient へ委譲し設定を束ねる | 入力: issue番号, tasks, options / 出力: IssueCreationResult |\n| `IssueClient` | フォローアップIssue生成の集約。LLM生成→フォールバック制御→Octokit呼び出し | 入力: tasks, context, options / 出力: タイトル・本文・ログ |\n| `IssueAIGenerator` (新規) | LLMプロンプト生成、API呼び出し、レスポンス検証 | 入力: tasks, context, options / 出力: { title, body, metadata } |\n| `LlmProviderAdapter` (OpenAI / Anthropic) | 各APIのラッパー。タイムアウト・再試行を実装 | 入出力: prompt, call options, completion JSON |\n| `config` / CLI | 環境変数・CLIから LLM 設定を収集し PhaseContextへ渡す | 入出力: Follow-up LLM 設定値 |\n| `SecretMasker` (既存) | 機密情報のマスキング | 入力: プロンプトPayload / 出力: SanitizedPayload |\n\n### 1.3 データフロー\n\n1. ユーザーが `ai-workflow execute ...` を実行し、CLI が Follow-up LLM オプションを解析 (デフォルトは無効)。\n2. `commands/execute` が `PhaseContext.issueGenerationOptions` を組み立て、`PhaseFactory` 経由で `EvaluationPhase` へ受け渡す。\n3. EvaluationPhase で残タスクが存在すると `GitHubClient.createIssueFromEvaluation(issueNumber, tasks, reportPath, context, options)` を呼び出す。\n4. `GitHubClient` は `IssueAIGenerator` をコンストラクタインジェクション済みの `IssueClient` に委譲。\n5. `IssueClient` が `options.enabled` と `IssueAIGenerator.isAvailable()` を確認し、利用可能なら `generate(tasks, context, options)` を試行。\n6. `IssueAIGenerator` は payload をサニタイズ→プロンプト生成→LLM呼び出し→検証し、成功時にタイトル・本文を返却。\n7. LLM 失敗または無効時、`IssueClient` は既存の `generateFollowUpTitle` と新設の `buildLegacyBody` でフォールバック本文を生成。\n8. 生成結果と `## 参考` セクション (＋オプションで LLM metadata) を結合し、Octokit で Issue を作成。ログへ成否とメタ情報を出力。\n\n### 1.4 主なシーケンスと失敗時動作\n\n- LLM 成功: `IssueAIGenerator` → validated result → `IssueClient` が LLM 出力を採用 → `options.appendMetadata` が true の場合にメタデータ節を付加。\n- LLM タイムアウト / レート制限: Providerアダプタが指数バックオフで再試行。全失敗で `IssueAIError` を返し IssueClient が WARN を記録しフォールバック。\n- プロンプト検証失敗: `IssueAIGenerator` が `IssueAIValidationError` を送出し、同様にフォールバック。\n- Octokit 失敗: 既存処理と同様に ERROR ログを出力し `IssueCreationResult` で失敗を返却。\n\n---\n\n## 2. 実装戦略判断: EXTEND\n\n**判断根拠**:\n- 既存 `IssueClient` / `GitHubClient` / CLI フローを維持したまま責務を拡張する必要があるため。\n- フォールバックとして既存テンプレートを保持しつつ LLM 生成を追加する形で後方互換を守る。\n- Planning Document の戦略 (新規モジュール追加 + 既存コード拡張) と整合。\n\n---\n\n## 3. テスト戦略判断: UNIT_INTEGRATION\n\n**判断根拠**:\n- プロンプト生成・レスポンス検証・リトライといったロジックはモック化が容易であり、ユニットテストで網羅できる。\n- GitHub 連携や Phase からのオプション伝搬、フォールバック全体の動作は統合テストで確認する必要がある。\n\n---\n\n## 4. テストコード戦略判断: BOTH_TEST\n\n**判断根拠**:\n- 既存 `issue-client` テストに LLM 成功/失敗パスを追加する必要がある (既存テストの拡張)。\n- `IssueAIGenerator` 用の専用ユニットテストが新規に必要となる (新規テスト作成)。\n\n---\n\n## 5. 影響範囲分析\n\n### 5.1 既存コードへの影響\n- `src/core/github/issue-client.ts`: 依存注入、LLM 分岐、本文生成をメソッド化、WARN/DEBUG ログ拡張。\n- `src/core/github-client.ts`: `IssueAIGenerator` の初期化と委譲。`createIssueFromEvaluation` にオプションパラメータ追加。\n- `src/core/phase-factory.ts`: `PhaseContext` へ追加した `issueGenerationOptions` を全 Phase に渡す初期化処理を拡張。\n- `src/phases/base-phase.ts`: Phase インスタンスが `issueGenerationOptions` を受け取り `this.context` へ保持できるようにする。\n- `src/phases/evaluation.ts`: `GitHubClient.createIssueFromEvaluation` 呼び出しに LLM オプションを渡す。\n- `src/core/secret-masker.ts`: `maskObject` を追加してネストした残タスクオブジェクトを一括マスキングできるようにする。\n- `src/commands/execute.ts` / `src/commands/execute/options-parser.ts`: CLI オプション解析に Follow-up LLM 設定を追加し `PhaseContext` へ渡す。\n- `src/types.ts`: `IssueGenerationOptions` や LLM 結果の型を追加。\n- `src/types/commands.ts`: `PhaseContext` に `issueGenerationOptions` プロパティを追加。\n- `src/core/config.ts`: LLM 設定用ゲッターを実装。\n- 既存テスト (`tests/unit/github/issue-client*.ts`, `tests/integration/github-client-facade.test.ts`, `tests/unit/secret-masker.test.ts`) を LLM 統合ケースとシークレットマスキング強化に合わせて更新。\n\n### 5.2 依存関係の変更\n- 追加パッケージは想定なし。既存 `openai`, `@anthropic-ai/claude-agent-sdk` を再利用。\n- Jest モックは既存 `jest-mock-extended` や手動モックを活用。\n\n### 5.3 マイグレーション要否\n- データマイグレーションは不要。\n- `.env.example` が存在する場合は Follow-up LLM 用環境変数を追加。\n- `ARCHITECTURE.md`, `CLAUDE.md`, `README.md` を更新して設定手順とフォールバック説明を追記。\n\n---\n\n## 6. 変更・追加ファイルリスト\n\n- **新規作成**\n  - `src/core/github/issue-ai-generator.ts`\n  - `tests/unit/github/issue-ai-generator.test.ts`\n  - `tests/integration/followup-issue-llm.test.ts` (Octokit モック中心)\n- **既存修正**\n  - `src/core/github/issue-client.ts`\n  - `src/core/github-client.ts`\n  - `src/core/phase-factory.ts`\n  - `src/phases/base-phase.ts`\n  - `src/phases/evaluation.ts`\n  - `src/core/secret-masker.ts`\n  - `src/commands/execute.ts`\n  - `src/commands/execute/options-parser.ts`\n  - `src/types.ts`\n  - `src/types/commands.ts`\n  - `src/core/config.ts`\n  - `tests/unit/github/issue-client.test.ts`\n  - `tests/unit/github/issue-client-followup.test.ts`\n  - `tests/unit/secret-masker.test.ts`\n  - `tests/integration/github-client-facade.test.ts`\n  - ドキュメント (`ARCHITECTURE.md`, `CLAUDE.md`, `README.md`, `.env.example`)\n- **削除予定**: なし\n\n---\n\n## 7. 詳細設計\n\n### 7.1 IssueGenerationOptions / IssueAIGenerationResult\n\n```ts\nexport interface IssueGenerationOptions {\n  enabled: boolean;\n  provider: 'auto' | 'openai' | 'claude';\n  model?: string;\n  temperature?: number;          // default 0.2\n  maxOutputTokens?: number;      // default 1500\n  timeoutMs?: number;            // default 25000\n  maxRetries?: number;           // default 3\n  maxTasks?: number;             // default 5\n  appendMetadata?: boolean;      // default false\n}\n\nexport interface IssueAIGenerationResult {\n  title: string;\n  body: string;\n  metadata: {\n    provider: 'openai' | 'claude';\n    model: string;\n    durationMs: number;\n    retryCount: number;\n    inputTokens?: number;\n    outputTokens?: number;\n    omittedTasks?: number;\n  };\n}\n```\n\n- `config` でデフォルトを構築し、CLI/環境変数で上書き可能にする。\n- `PhaseContext` に `issueGenerationOptions` を追加し、省略時は `{ enabled: false, provider: 'auto' }` を適用。\n\n### 7.2 IssueAIGenerator クラス (新規)\n\n- コンストラクタ: `(providers: Record<'openai' | 'claude', LlmProviderAdapter>, secretMasker = new SecretMasker())`。\n- 補助的なエラー型:\n  - `IssueAIUnavailableError` (credentials 不足など)\n  - `IssueAIValidationError` (出力検証失敗)\n- 公開メソッド:\n  - `isAvailable(options: IssueGenerationOptions): boolean`  \n    - `options.enabled` が true かつ選択された provider（`auto` の場合は利用可能なもの）が `hasCredentials()` を満たす。\n  - `generate(tasks, context, issueNumber, options): Promise<IssueAIGenerationResult>`  \n    1. `sanitizePayload(tasks, context, options.maxTasks ?? 5)`  \n       - `RemainingTask` を優先度順 (High→Medium→Low) に並べ、上位 `maxTasks` を採用。超過分は `omittedTasks` としてメタに記録。  \n       - 各文字列フィールドは 512 文字にトリム。`targetFiles` は 10 件、`steps` / `acceptanceCriteria` は各 8 件まで。  \n       - `SecretMasker.maskObject` で既知のシークレット値・トークン・メールアドレスを `[REDACTED_x]` に置換。\n    2. `buildPrompt(issueNumber, sanitizedPayload, context, options)`  \n       - JSON 文字列化し、テンプレートへ埋め込む。\n    3. `invokeProvider(prompt, options)`  \n       - 選択された provider の `complete()` を呼び出す。  \n       - レート制限時は指数バックオフ (2000ms, 4000ms, 8000ms) とし、回数は `options.maxRetries`。\n    4. `parseAndValidate(responseText)`  \n       - JSON パース → タイトル長 50〜80 文字 → 必須セクションを順番に確認 → `実行内容` セクションが番号付きリスト (`1.` 形式) とテスト手順 (`テスト` or `検証`) を含むか検証 → HTML タグを禁止。  \n       - 失敗時は `IssueAIValidationError`。\n    5. 成功時に metadata (provider, model, duration, retryCount, input/output tokens, omittedTasks) 付きで返却。\n\n### 7.3 LlmProviderAdapter\n\n```ts\ninterface LlmProviderAdapter {\n  name: 'openai' | 'claude';\n  hasCredentials(): boolean;\n  complete(prompt: string, options: IssueGenerationOptions): Promise<LlmProviderResponse>;\n}\n\ninterface LlmProviderResponse {\n  text: string;\n  inputTokens?: number;\n  outputTokens?: number;\n  retryCount: number;\n  durationMs: number;\n}\n```\n\n- **OpenAIAdapter**\n  - `openai.chat.completions.create()` を呼び出し、`response_format: { type: 'json_object' }` を指定。\n  - `AbortController` で `timeoutMs` を強制。\n  - HTTP 429/5xx 時は指数バックオフで再試行。最終的に失敗ならエラーをスロー。\n- **AnthropicAdapter**\n  - `@anthropic-ai/claude-agent-sdk` の `messages.create()` を利用。\n  - `options.model` が無ければ `claude-3-sonnet-20240229` を使用。\n  - 応答の `content` を結合し JSON テキストを取得。\n- `provider: 'auto'` の場合は OpenAI キーが存在すれば OpenAIAdapter、それ以外は ClaudeAdapter を選択。\n\n### 7.4 プロンプト生成とバリデーション\n\nテンプレート例 (コード内定数として保持):\n\n````markdown\nあなたはソフトウェア開発プロジェクトのIssue作成アシスタントです。\n以下のJSONを読み取り、フォローアップIssueを構築してください。\n\n入力:\n{{payload}}\n\n要件:\n1. タイトルは50〜80文字。対象コンポーネントや目的のキーワードを含めること。\n2. 本文は以下の見出し順序とします。\n   ## 背景\n   ## 目的\n   ## 実行内容\n   ## 受け入れ基準\n   ## 関連リソース\n3. 実行内容には対象ファイル・手順・テスト方法を含めること。\n4. JSON 形式で回答してください。\n\n出力形式:\n{\n  \"title\": \"...\",\n  \"body\": \"...\"\n}\n````\n\n- `parseAndValidate` 検証ルール:\n  - JSON パース失敗 → `IssueAIValidationError`。\n  - タイトル文字数 (全角半角問わず) が 50 未満または 80 超過で失敗。\n  - 本文に必須5セクションが順番に存在するか正規表現で確認。\n  - `## 実行内容` 内に番号付きリスト (`^\\d+\\. `) があり、いずれかの行に `テスト`/`検証` を含むことを確認。\n  - HTML/スクリプトタグを検出したら失敗。\n  - 余分な末尾空行は `trimEnd()` で整理。\n\n### 7.5 IssueClient 拡張\n\n- コンストラクタに `IssueAIGenerator | null` を追加 (`new IssueClient(octokit, owner, repo, issueAIGenerator)`).\n- 新規ヘルパー:\n  - `private buildLegacyBody(...)`: 現行ロジックを抽出し、フォールバック時に再利用。\n  - `private appendMetadata(body, metadata, options)`: `options.appendMetadata` が true の場合に以下を追加。\n    ```\n    ## 生成メタデータ\n    - モデル: ${metadata.model} (${metadata.provider})\n    - 所要時間: ${metadata.durationMs}ms / 再試行: ${metadata.retryCount}\n    - トークン: in ${metadata.inputTokens ?? '-'} / out ${metadata.outputTokens ?? '-'}\n    - 省略したタスク数: ${metadata.omittedTasks ?? 0}\n    ```\n  - `private async tryGenerateWithLLM(...)`: LLM が利用可能か判定し、失敗時は WARN ログで理由を記録して `null` を返す。\n- `createIssueFromEvaluation` の流れ:\n  1. `const aiResult = await this.tryGenerateWithLLM(...);`\n  2. `const title = aiResult?.title ?? this.generateFollowUpTitle(...);`\n  3. `const baseBody = aiResult?.body ?? this.buildLegacyBody(...);`\n  4. `const body = aiResult ? this.appendMetadata(baseBody, aiResult.metadata, options) : baseBody;`\n  5. 既存どおり Octokit で Issue を作成。\n- ログ出力:\n  - 成功 (`logger.debug`): `FOLLOWUP_LLM_SUCCESS { provider, model, durationMs, retryCount }`\n  - フォールバック (`logger.warn`): `FOLLOWUP_LLM_FALLBACK { reason, fallback: 'legacy_template' }`\n  - ログにはプロンプト本文を含めない。\n\n### 7.6 GitHubClient / Phase 連携\n\n- `GitHubClient` コンストラクタで `IssueAIGenerator` を生成し `IssueClient` に渡す。\n- `createIssueFromEvaluation` の署名を `(..., issueContext?: IssueContext, options?: IssueGenerationOptions)` に拡張。`options` が無い場合は `config` から取得する。\n- `EvaluationPhase`:\n  ```ts\n  const options = this.context.issueGenerationOptions ?? { enabled: false, provider: 'auto' };\n  const result = await this.github.createIssueFromEvaluation(\n    issueNumber,\n    remainingTasks,\n    relativeReportPath,\n    issueContext,\n    options,\n  );\n  ```\n- `PhaseFactory` / `BasePhase` で `PhaseContext.issueGenerationOptions` を新たに受け渡す。\n\n### 7.7 CLI / Config 拡張\n\n- `ExecuteCommandOptions` に以下フィールドを追加:\n  - `followupLlmMode?: 'auto' | 'openai' | 'claude' | 'off'`\n  - `followupLlmModel?: string`\n  - `followupLlmTimeout?: number`\n  - `followupLlmMaxRetries?: number`\n  - `followupLlmAppendMetadata?: boolean`\n- `options-parser.ts` でバリデーション:\n  - `off` → `enabled` false。\n  - timeout/retries は正の整数 (0 許容)。\n  - provider 指定が `openai` なのに OpenAI APIキー不在の場合は警告ログを出して `enabled=false`。\n- `config.ts` で環境変数ゲッターを追加 (`FOLLOWUP_LLM_MODE`, `FOLLOWUP_LLM_MODEL`, `FOLLOWUP_LLM_TIMEOUT_MS`, `FOLLOWUP_LLM_MAX_RETRIES`, `FOLLOWUP_LLM_APPEND_METADATA`)。\n- CLI 例:  \n  `ai-workflow execute --issue 119 --phase evaluation --followup-llm-mode auto --followup-llm-model claude-3-sonnet-20240229`.\n\n### 7.8 SecretMasker 拡張\n\n- 新規メソッド `maskObject<T>(input: T, options?: { ignoredPaths?: string[] }): T` を追加し、入力オブジェクトを破壊せずに深いコピーを返す。`ignoredPaths` は `['tasks.*.metadata']` のようなドット表記で除外を指定できる。\n- 処理フロー:\n  1. `getSecretList()` で環境変数ベースのシークレットを取得し、`[REDACTED_${name}]` への置換テーブルを構築。\n  2. 追加で以下のパターンを検出する正規表現を用意し、ヒットした文字列は `[REDACTED_PATTERN]` に置換する。  \n     - 長さ 20 文字以上の英数字+`-_` 混在トークン (`/[A-Za-z0-9_-]{20,}/g`)  \n     - メールアドレス (`/[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}/g`)  \n     - `Bearer <token>` / `token=` 形式 (`/(Bearer|token=)[\\w\\-.]+/gi`)\n  3. 走査は DFS で実装し、`WeakSet` で循環参照を検出して二度処理しない。配列は同じく再帰し、プリミティブ以外は `Object.prototype.toString` で plain object のみを対象にする。\n  4. 文字列に対しては上記パターンとシークレット値を順次 `replaceAll` し、オブジェクト／配列はフィールド単位で再帰結果を集約する。\n- 戻り値は元の構造を維持した `sanitized` コピー。元のオブジェクトは変更せず、`undefined` や関数はそのまま返す。\n- `IssueAIGenerator.sanitizePayload` はこの `maskObject` の戻り値に対して文字数トリムやタスク数制限を適用し、マスキングと整形の責務を分離する。\n- 追加するユニットテストでは、ネストしたオブジェクトと配列、循環参照、`ignoredPaths` 指定時の除外、メールアドレス置換が期待通り動くことを確認する。\n\n### 7.9 ロギング・モニタリング\n\n- LLM 成功時は DEBUG ログ、再試行数 > 0 の場合は WARN と INFO の両方に出力して追跡可能にする。\n- フォールバック発生時は WARN ログを構造化文字列 (JSON 互換) で出力。`event=FOLLOWUP_LLM_FALLBACK`, `fallback_mode=legacy_template`, `reason=...`。\n- `IssueAIGenerator.generate` 内で `performance.now()` を使い処理時間を計測。\n- ログには機密情報やプロンプト全文を含めない。\n\n### 7.10 テスト設計詳細\n\n| レイヤ | テストケース | 目的 |\n| --- | --- | --- |\n| Unit (`issue-ai-generator.test.ts`) | タスク数制限・文字列トリム・ターゲットファイル上限を検証 | サニタイズ仕様の担保 |\n|  | JSON 以外の応答で `IssueAIValidationError` を投げる | バリデーション |\n|  | 必須セクション欠落、タイトル長不正、HTMLタグ混入で失敗する | FR-1/FR-2 |\n|  | 1回目失敗→2回目成功のリトライ時に最終成功 | リトライ制御 |\n|  | `SecretMasker` により API キーがプロンプトに残らない | セキュリティ |\n| Unit (`secret-masker.test.ts`) | `maskObject` がネスト構造・配列・循環参照を安全にマスキングする | サニタイズ機構の信頼性 |\n|  | `ignoredPaths` 指定時に該当フィールドをスキップしつつ他をマスクする | 柔軟な除外設定 |\n| Unit (`issue-client.test.ts`) | LLM 成功時に Octokit へ LLM 出力が渡る | フロー検証 |\n|  | 例外発生時に WARN ログとフォールバックタイトル/本文が使用される | FR-3 |\n| Integration (`followup-issue-llm.test.ts`) | CLI -> PhaseContext -> GitHubClient -> IssueClient のオプション伝搬 | 設定連携 |\n|  | LLM が無効化されている場合に既存挙動が維持される | 後方互換 |\n|  | `FOLLOWUP_LLM_E2E=1` 時のみ実APIを使い、成功時タイトル/本文が要件を満たすか検証 (失敗時はテストをスキップ) | 実API検証 |\n\n### 7.11 要件トレーサビリティ\n\n| 要件ID | 対応箇所 |\n| --- | --- |\n| FR-1 | 7.4 プロンプト設計・タイトル検証、7.5 タイトル採用ロジック |\n| FR-2 | 7.4 セクション検証、7.5 `buildLegacyBody` との比較で差異を吸収 |\n| FR-3 | 7.5 `tryGenerateWithLLM` フォールバック制御 |\n| FR-4 | 7.6 Phase 連携、7.7 CLI/Config 拡張 |\n| FR-5 | 7.5 ログ出力設計、7.9 モニタリング |\n\n### 7.12 ドキュメント更新\n\n- `ARCHITECTURE.md`: Evaluation → GitHubClient → IssueAIGenerator → IssueClient のフロー図と説明を追加。\n- `CLAUDE.md`: Follow-up LLM 設定方法、環境変数、フォールバック観察ポイントを追記。\n- `README.md`: CLI オプションと `.env` 設定例、フォールバック時のトラブルシューティングを追加。\n\n---\n\n## 8. セキュリティ考慮事項\n\n- `SecretMasker` と追加の簡易正規表現 (API キーフォーマット、メールアドレス) を `sanitizePayload` に適用し、機密情報送信を防止。\n- LLM 応答に HTML/スクリプトタグが含まれる場合はバリデーションエラーとし、フォールバックへ切り替える。\n- API キーは `config` ゲッター経由でのみ参照し、ログへ出力しない。\n- プロンプト・レスポンスをファイルへ書き出さない。ログにはメタデータのみを残す。\n- 再試行回数を `maxRetries` で制限し、無限ループやコスト過多を防止。\n\n---\n\n## 9. 非機能要件への対応\n\n- **パフォーマンス**: `timeoutMs` と `maxRetries` で最悪ケースでも 25s × 3 = 75s 以内にフォールバック。タスク数・文字数の制限で入力サイズを抑制し、LLM 呼び出し平均 15s 以内を目指す。\n- **スケーラビリティ**: Provider 抽象化でモデル追加が容易。`maxTasks` で大規模残タスクでも安定して処理。\n- **保守性**: LLM ロジックを `IssueAIGenerator` に集約し、IssueClient と疎結合化。テストで回帰を検知しやすくする。\n- **可用性**: LLM 失敗時でも既存テンプレートで確実に Issue を生成 (FR-3)。WARN ログで運用監視が容易。\n- **コスト管理**: デフォルト無効 (`enabled=false`) で不要な API 呼び出しを防止。`appendMetadata` で生成コストを Issue 上に可視化可能。\n\n---\n\n## 10. 実装の順序\n\n1. **型と設定の整備**: `IssueGenerationOptions`、`PhaseContext`、CLI/Config 拡張。既存コードをコンパイル可能に更新。\n2. **IssueAIGenerator 実装**: プロンプトテンプレート、サニタイズ、Provider アダプタ、検証、専用エラーを実装。\n3. **IssueClient / GitHubClient 更新**: 依存注入、LLM 分岐、フォールバックとログ処理を追加。\n4. **Phase / CLI 連携**: EvaluationPhase がオプションを渡すよう調整し、実行時の設定反映を確認。\n5. **テスト追加・更新**: 新規ユニットテスト、既存テスト更新、統合テストでオプション伝搬とフォールバックを検証。\n6. **ドキュメント更新**: ARCHITECTURE / CLAUDE / README / `.env.example` を更新。\n7. **検証**: `npm run test:unit`, `npm run test:integration` 実行。必要に応じ `FOLLOWUP_LLM_E2E=1` で手動統合テストを確認。\n\n---\n\n## 11. 品質ゲート確認\n\n- 実装戦略 (EXTEND) の判断根拠を明記。\n- テスト戦略 (UNIT_INTEGRATION) の判断根拠を明記。\n- 既存コードへの影響と依存関係を分析。\n- 変更・追加ファイルを列挙。\n- 詳細設計と要件トレーサビリティを提示し実装可能性を保証。\n","exit_code":0,"status":"completed"}}
{"type":"item.started","item":{"id":"item_10","type":"command_execution","command":"bash -lc \"sed -n '150,220p' .ai-workflow/issue-119/02_design/output/design.md\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_10","type":"command_execution","command":"bash -lc \"sed -n '150,220p' .ai-workflow/issue-119/02_design/output/design.md\"","aggregated_output":"  maxRetries?: number;           // default 3\n  maxTasks?: number;             // default 5\n  appendMetadata?: boolean;      // default false\n}\n\nexport interface IssueAIGenerationResult {\n  title: string;\n  body: string;\n  metadata: {\n    provider: 'openai' | 'claude';\n    model: string;\n    durationMs: number;\n    retryCount: number;\n    inputTokens?: number;\n    outputTokens?: number;\n    omittedTasks?: number;\n  };\n}\n```\n\n- `config` でデフォルトを構築し、CLI/環境変数で上書き可能にする。\n- `PhaseContext` に `issueGenerationOptions` を追加し、省略時は `{ enabled: false, provider: 'auto' }` を適用。\n\n### 7.2 IssueAIGenerator クラス (新規)\n\n- コンストラクタ: `(providers: Record<'openai' | 'claude', LlmProviderAdapter>, secretMasker = new SecretMasker())`。\n- 補助的なエラー型:\n  - `IssueAIUnavailableError` (credentials 不足など)\n  - `IssueAIValidationError` (出力検証失敗)\n- 公開メソッド:\n  - `isAvailable(options: IssueGenerationOptions): boolean`  \n    - `options.enabled` が true かつ選択された provider（`auto` の場合は利用可能なもの）が `hasCredentials()` を満たす。\n  - `generate(tasks, context, issueNumber, options): Promise<IssueAIGenerationResult>`  \n    1. `sanitizePayload(tasks, context, options.maxTasks ?? 5)`  \n       - `RemainingTask` を優先度順 (High→Medium→Low) に並べ、上位 `maxTasks` を採用。超過分は `omittedTasks` としてメタに記録。  \n       - 各文字列フィールドは 512 文字にトリム。`targetFiles` は 10 件、`steps` / `acceptanceCriteria` は各 8 件まで。  \n       - `SecretMasker.maskObject` で既知のシークレット値・トークン・メールアドレスを `[REDACTED_x]` に置換。\n    2. `buildPrompt(issueNumber, sanitizedPayload, context, options)`  \n       - JSON 文字列化し、テンプレートへ埋め込む。\n    3. `invokeProvider(prompt, options)`  \n       - 選択された provider の `complete()` を呼び出す。  \n       - レート制限時は指数バックオフ (2000ms, 4000ms, 8000ms) とし、回数は `options.maxRetries`。\n    4. `parseAndValidate(responseText)`  \n       - JSON パース → タイトル長 50〜80 文字 → 必須セクションを順番に確認 → `実行内容` セクションが番号付きリスト (`1.` 形式) とテスト手順 (`テスト` or `検証`) を含むか検証 → HTML タグを禁止。  \n       - 失敗時は `IssueAIValidationError`。\n    5. 成功時に metadata (provider, model, duration, retryCount, input/output tokens, omittedTasks) 付きで返却。\n\n### 7.3 LlmProviderAdapter\n\n```ts\ninterface LlmProviderAdapter {\n  name: 'openai' | 'claude';\n  hasCredentials(): boolean;\n  complete(prompt: string, options: IssueGenerationOptions): Promise<LlmProviderResponse>;\n}\n\ninterface LlmProviderResponse {\n  text: string;\n  inputTokens?: number;\n  outputTokens?: number;\n  retryCount: number;\n  durationMs: number;\n}\n```\n\n- **OpenAIAdapter**\n  - `openai.chat.completions.create()` を呼び出し、`response_format: { type: 'json_object' }` を指定。\n  - `AbortController` で `timeoutMs` を強制。\n  - HTTP 429/5xx 時は指数バックオフで再試行。最終的に失敗ならエラーをスロー。\n- **AnthropicAdapter**\n  - `@anthropic-ai/claude-agent-sdk` の `messages.create()` を利用。\n","exit_code":0,"status":"completed"}}
{"type":"item.started","item":{"id":"item_11","type":"command_execution","command":"bash -lc \"sed -n '220,320p' .ai-workflow/issue-119/02_design/output/design.md\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_11","type":"command_execution","command":"bash -lc \"sed -n '220,320p' .ai-workflow/issue-119/02_design/output/design.md\"","aggregated_output":"  - `@anthropic-ai/claude-agent-sdk` の `messages.create()` を利用。\n  - `options.model` が無ければ `claude-3-sonnet-20240229` を使用。\n  - 応答の `content` を結合し JSON テキストを取得。\n- `provider: 'auto'` の場合は OpenAI キーが存在すれば OpenAIAdapter、それ以外は ClaudeAdapter を選択。\n\n### 7.4 プロンプト生成とバリデーション\n\nテンプレート例 (コード内定数として保持):\n\n````markdown\nあなたはソフトウェア開発プロジェクトのIssue作成アシスタントです。\n以下のJSONを読み取り、フォローアップIssueを構築してください。\n\n入力:\n{{payload}}\n\n要件:\n1. タイトルは50〜80文字。対象コンポーネントや目的のキーワードを含めること。\n2. 本文は以下の見出し順序とします。\n   ## 背景\n   ## 目的\n   ## 実行内容\n   ## 受け入れ基準\n   ## 関連リソース\n3. 実行内容には対象ファイル・手順・テスト方法を含めること。\n4. JSON 形式で回答してください。\n\n出力形式:\n{\n  \"title\": \"...\",\n  \"body\": \"...\"\n}\n````\n\n- `parseAndValidate` 検証ルール:\n  - JSON パース失敗 → `IssueAIValidationError`。\n  - タイトル文字数 (全角半角問わず) が 50 未満または 80 超過で失敗。\n  - 本文に必須5セクションが順番に存在するか正規表現で確認。\n  - `## 実行内容` 内に番号付きリスト (`^\\d+\\. `) があり、いずれかの行に `テスト`/`検証` を含むことを確認。\n  - HTML/スクリプトタグを検出したら失敗。\n  - 余分な末尾空行は `trimEnd()` で整理。\n\n### 7.5 IssueClient 拡張\n\n- コンストラクタに `IssueAIGenerator | null` を追加 (`new IssueClient(octokit, owner, repo, issueAIGenerator)`).\n- 新規ヘルパー:\n  - `private buildLegacyBody(...)`: 現行ロジックを抽出し、フォールバック時に再利用。\n  - `private appendMetadata(body, metadata, options)`: `options.appendMetadata` が true の場合に以下を追加。\n    ```\n    ## 生成メタデータ\n    - モデル: ${metadata.model} (${metadata.provider})\n    - 所要時間: ${metadata.durationMs}ms / 再試行: ${metadata.retryCount}\n    - トークン: in ${metadata.inputTokens ?? '-'} / out ${metadata.outputTokens ?? '-'}\n    - 省略したタスク数: ${metadata.omittedTasks ?? 0}\n    ```\n  - `private async tryGenerateWithLLM(...)`: LLM が利用可能か判定し、失敗時は WARN ログで理由を記録して `null` を返す。\n- `createIssueFromEvaluation` の流れ:\n  1. `const aiResult = await this.tryGenerateWithLLM(...);`\n  2. `const title = aiResult?.title ?? this.generateFollowUpTitle(...);`\n  3. `const baseBody = aiResult?.body ?? this.buildLegacyBody(...);`\n  4. `const body = aiResult ? this.appendMetadata(baseBody, aiResult.metadata, options) : baseBody;`\n  5. 既存どおり Octokit で Issue を作成。\n- ログ出力:\n  - 成功 (`logger.debug`): `FOLLOWUP_LLM_SUCCESS { provider, model, durationMs, retryCount }`\n  - フォールバック (`logger.warn`): `FOLLOWUP_LLM_FALLBACK { reason, fallback: 'legacy_template' }`\n  - ログにはプロンプト本文を含めない。\n\n### 7.6 GitHubClient / Phase 連携\n\n- `GitHubClient` コンストラクタで `IssueAIGenerator` を生成し `IssueClient` に渡す。\n- `createIssueFromEvaluation` の署名を `(..., issueContext?: IssueContext, options?: IssueGenerationOptions)` に拡張。`options` が無い場合は `config` から取得する。\n- `EvaluationPhase`:\n  ```ts\n  const options = this.context.issueGenerationOptions ?? { enabled: false, provider: 'auto' };\n  const result = await this.github.createIssueFromEvaluation(\n    issueNumber,\n    remainingTasks,\n    relativeReportPath,\n    issueContext,\n    options,\n  );\n  ```\n- `PhaseFactory` / `BasePhase` で `PhaseContext.issueGenerationOptions` を新たに受け渡す。\n\n### 7.7 CLI / Config 拡張\n\n- `ExecuteCommandOptions` に以下フィールドを追加:\n  - `followupLlmMode?: 'auto' | 'openai' | 'claude' | 'off'`\n  - `followupLlmModel?: string`\n  - `followupLlmTimeout?: number`\n  - `followupLlmMaxRetries?: number`\n  - `followupLlmAppendMetadata?: boolean`\n- `options-parser.ts` でバリデーション:\n  - `off` → `enabled` false。\n  - timeout/retries は正の整数 (0 許容)。\n  - provider 指定が `openai` なのに OpenAI APIキー不在の場合は警告ログを出して `enabled=false`。\n- `config.ts` で環境変数ゲッターを追加 (`FOLLOWUP_LLM_MODE`, `FOLLOWUP_LLM_MODEL`, `FOLLOWUP_LLM_TIMEOUT_MS`, `FOLLOWUP_LLM_MAX_RETRIES`, `FOLLOWUP_LLM_APPEND_METADATA`)。\n- CLI 例:  \n  `ai-workflow execute --issue 119 --phase evaluation --followup-llm-mode auto --followup-llm-model claude-3-sonnet-20240229`.\n\n### 7.8 SecretMasker 拡張\n","exit_code":0,"status":"completed"}}
{"type":"item.started","item":{"id":"item_12","type":"command_execution","command":"bash -lc \"sed -n '320,420p' .ai-workflow/issue-119/02_design/output/design.md\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_12","type":"command_execution","command":"bash -lc \"sed -n '320,420p' .ai-workflow/issue-119/02_design/output/design.md\"","aggregated_output":"### 7.8 SecretMasker 拡張\n\n- 新規メソッド `maskObject<T>(input: T, options?: { ignoredPaths?: string[] }): T` を追加し、入力オブジェクトを破壊せずに深いコピーを返す。`ignoredPaths` は `['tasks.*.metadata']` のようなドット表記で除外を指定できる。\n- 処理フロー:\n  1. `getSecretList()` で環境変数ベースのシークレットを取得し、`[REDACTED_${name}]` への置換テーブルを構築。\n  2. 追加で以下のパターンを検出する正規表現を用意し、ヒットした文字列は `[REDACTED_PATTERN]` に置換する。  \n     - 長さ 20 文字以上の英数字+`-_` 混在トークン (`/[A-Za-z0-9_-]{20,}/g`)  \n     - メールアドレス (`/[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}/g`)  \n     - `Bearer <token>` / `token=` 形式 (`/(Bearer|token=)[\\w\\-.]+/gi`)\n  3. 走査は DFS で実装し、`WeakSet` で循環参照を検出して二度処理しない。配列は同じく再帰し、プリミティブ以外は `Object.prototype.toString` で plain object のみを対象にする。\n  4. 文字列に対しては上記パターンとシークレット値を順次 `replaceAll` し、オブジェクト／配列はフィールド単位で再帰結果を集約する。\n- 戻り値は元の構造を維持した `sanitized` コピー。元のオブジェクトは変更せず、`undefined` や関数はそのまま返す。\n- `IssueAIGenerator.sanitizePayload` はこの `maskObject` の戻り値に対して文字数トリムやタスク数制限を適用し、マスキングと整形の責務を分離する。\n- 追加するユニットテストでは、ネストしたオブジェクトと配列、循環参照、`ignoredPaths` 指定時の除外、メールアドレス置換が期待通り動くことを確認する。\n\n### 7.9 ロギング・モニタリング\n\n- LLM 成功時は DEBUG ログ、再試行数 > 0 の場合は WARN と INFO の両方に出力して追跡可能にする。\n- フォールバック発生時は WARN ログを構造化文字列 (JSON 互換) で出力。`event=FOLLOWUP_LLM_FALLBACK`, `fallback_mode=legacy_template`, `reason=...`。\n- `IssueAIGenerator.generate` 内で `performance.now()` を使い処理時間を計測。\n- ログには機密情報やプロンプト全文を含めない。\n\n### 7.10 テスト設計詳細\n\n| レイヤ | テストケース | 目的 |\n| --- | --- | --- |\n| Unit (`issue-ai-generator.test.ts`) | タスク数制限・文字列トリム・ターゲットファイル上限を検証 | サニタイズ仕様の担保 |\n|  | JSON 以外の応答で `IssueAIValidationError` を投げる | バリデーション |\n|  | 必須セクション欠落、タイトル長不正、HTMLタグ混入で失敗する | FR-1/FR-2 |\n|  | 1回目失敗→2回目成功のリトライ時に最終成功 | リトライ制御 |\n|  | `SecretMasker` により API キーがプロンプトに残らない | セキュリティ |\n| Unit (`secret-masker.test.ts`) | `maskObject` がネスト構造・配列・循環参照を安全にマスキングする | サニタイズ機構の信頼性 |\n|  | `ignoredPaths` 指定時に該当フィールドをスキップしつつ他をマスクする | 柔軟な除外設定 |\n| Unit (`issue-client.test.ts`) | LLM 成功時に Octokit へ LLM 出力が渡る | フロー検証 |\n|  | 例外発生時に WARN ログとフォールバックタイトル/本文が使用される | FR-3 |\n| Integration (`followup-issue-llm.test.ts`) | CLI -> PhaseContext -> GitHubClient -> IssueClient のオプション伝搬 | 設定連携 |\n|  | LLM が無効化されている場合に既存挙動が維持される | 後方互換 |\n|  | `FOLLOWUP_LLM_E2E=1` 時のみ実APIを使い、成功時タイトル/本文が要件を満たすか検証 (失敗時はテストをスキップ) | 実API検証 |\n\n### 7.11 要件トレーサビリティ\n\n| 要件ID | 対応箇所 |\n| --- | --- |\n| FR-1 | 7.4 プロンプト設計・タイトル検証、7.5 タイトル採用ロジック |\n| FR-2 | 7.4 セクション検証、7.5 `buildLegacyBody` との比較で差異を吸収 |\n| FR-3 | 7.5 `tryGenerateWithLLM` フォールバック制御 |\n| FR-4 | 7.6 Phase 連携、7.7 CLI/Config 拡張 |\n| FR-5 | 7.5 ログ出力設計、7.9 モニタリング |\n\n### 7.12 ドキュメント更新\n\n- `ARCHITECTURE.md`: Evaluation → GitHubClient → IssueAIGenerator → IssueClient のフロー図と説明を追加。\n- `CLAUDE.md`: Follow-up LLM 設定方法、環境変数、フォールバック観察ポイントを追記。\n- `README.md`: CLI オプションと `.env` 設定例、フォールバック時のトラブルシューティングを追加。\n\n---\n\n## 8. セキュリティ考慮事項\n\n- `SecretMasker` と追加の簡易正規表現 (API キーフォーマット、メールアドレス) を `sanitizePayload` に適用し、機密情報送信を防止。\n- LLM 応答に HTML/スクリプトタグが含まれる場合はバリデーションエラーとし、フォールバックへ切り替える。\n- API キーは `config` ゲッター経由でのみ参照し、ログへ出力しない。\n- プロンプト・レスポンスをファイルへ書き出さない。ログにはメタデータのみを残す。\n- 再試行回数を `maxRetries` で制限し、無限ループやコスト過多を防止。\n\n---\n\n## 9. 非機能要件への対応\n\n- **パフォーマンス**: `timeoutMs` と `maxRetries` で最悪ケースでも 25s × 3 = 75s 以内にフォールバック。タスク数・文字数の制限で入力サイズを抑制し、LLM 呼び出し平均 15s 以内を目指す。\n- **スケーラビリティ**: Provider 抽象化でモデル追加が容易。`maxTasks` で大規模残タスクでも安定して処理。\n- **保守性**: LLM ロジックを `IssueAIGenerator` に集約し、IssueClient と疎結合化。テストで回帰を検知しやすくする。\n- **可用性**: LLM 失敗時でも既存テンプレートで確実に Issue を生成 (FR-3)。WARN ログで運用監視が容易。\n- **コスト管理**: デフォルト無効 (`enabled=false`) で不要な API 呼び出しを防止。`appendMetadata` で生成コストを Issue 上に可視化可能。\n\n---\n\n## 10. 実装の順序\n\n1. **型と設定の整備**: `IssueGenerationOptions`、`PhaseContext`、CLI/Config 拡張。既存コードをコンパイル可能に更新。\n2. **IssueAIGenerator 実装**: プロンプトテンプレート、サニタイズ、Provider アダプタ、検証、専用エラーを実装。\n3. **IssueClient / GitHubClient 更新**: 依存注入、LLM 分岐、フォールバックとログ処理を追加。\n4. **Phase / CLI 連携**: EvaluationPhase がオプションを渡すよう調整し、実行時の設定反映を確認。\n5. **テスト追加・更新**: 新規ユニットテスト、既存テスト更新、統合テストでオプション伝搬とフォールバックを検証。\n6. **ドキュメント更新**: ARCHITECTURE / CLAUDE / README / `.env.example` を更新。\n7. **検証**: `npm run test:unit`, `npm run test:integration` 実行。必要に応じ `FOLLOWUP_LLM_E2E=1` で手動統合テストを確認。\n\n---\n\n## 11. 品質ゲート確認\n\n- 実装戦略 (EXTEND) の判断根拠を明記。\n- テスト戦略 (UNIT_INTEGRATION) の判断根拠を明記。\n- 既存コードへの影響と依存関係を分析。\n- 変更・追加ファイルを列挙。\n- 詳細設計と要件トレーサビリティを提示し実装可能性を保証。\n","exit_code":0,"status":"completed"}}
{"type":"item.updated","item":{"id":"item_0","type":"todo_list","items":[{"text":"Review planning, requirements, and design documents to understand scope and test strategy","completed":true},{"text":"Derive unit and integration test scenarios aligned with requirements and design","completed":false},{"text":"Draft and save test scenario markdown document at .ai-workflow/issue-119/03_test_scenario/output/test-scenario.md","completed":false}]}}
{"type":"item.started","item":{"id":"item_13","type":"command_execution","command":"bash -lc 'mkdir -p .ai-workflow/issue-119/03_test_scenario/output'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_13","type":"command_execution","command":"bash -lc 'mkdir -p .ai-workflow/issue-119/03_test_scenario/output'","aggregated_output":"","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_14","type":"file_change","changes":[{"path":"/tmp/jenkins-33ebb7ab/workspace/AI_Workflow/ai_workflow_orchestrator_develop/.ai-workflow/issue-119/03_test_scenario/output/test-scenario.md","kind":"add"}],"status":"completed"}}
{"type":"item.started","item":{"id":"item_15","type":"command_execution","command":"bash -lc 'cat .ai-workflow/issue-119/03_test_scenario/output/test-scenario.md'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_15","type":"command_execution","command":"bash -lc 'cat .ai-workflow/issue-119/03_test_scenario/output/test-scenario.md'","aggregated_output":"# テストシナリオ - Issue #119 フォローアップIssue生成品質の改善（LLM活用）\n\n## 1. テスト戦略サマリー\n- **戦略**: UNIT_INTEGRATION（Phase 2確定）。ユニットテストで `IssueAIGenerator` / `IssueClient` / `SecretMasker` のロジックを網羅し、統合テストで CLI→Phase→GitHubClient の連携とフォールバック挙動を検証する。\n- **テスト対象範囲**: 新規 `issue-ai-generator.ts`、拡張された `issue-client.ts`、`secret-masker` 拡張、CLI オプションと PhaseContext の連携、LLM プロバイダアダプタ。\n- **テスト目的**: FR-1〜FR-5 に沿って LLM 生成タイトル/本文品質、フォールバック制御、設定オプション伝搬、ログ/メタデータ記録、セキュリティ（マスキング）を確認する。\n\n## 2. Unitテストシナリオ\n\n### テストケース名: issue_ai_generator_generate_success_正常系\n- 目的: LLM プロバイダが有効な JSON を返却した際に、タイトル/本文/メタデータが採用されることを検証する（FR-1, FR-2, FR-5）。\n- 前提条件: `IssueGenerationOptions.enabled=true`, provider は `openai`, API キーはモックで利用可能。プロバイダは1回で成功レスポンスを返す。\n- 入力: 高優先度タスク1件、`IssueContext`（summary, blockerStatus, deferredReason）、`maxTasks=3`, `appendMetadata=true`。\n- 期待結果: 50〜80文字のタイトルと5セクションを含む本文を返却。メタデータに provider/model/duration/retryCount=0 が設定され、`appendMetadata` 指定時に本文末尾へ追記される。\n- テストデータ: `task_high_priority`（後述）、`context_with_blocker`、モックレスポンス `{ \"title\": \"カバレッジ90%達成 - core/gitの単体テスト拡張\", \"body\": \"## 背景\\n...\" }`。\n\n### テストケース名: issue_ai_generator_generate_retry_success_正常系\n- 目的: プロバイダが一時的に失敗した場合でも最大リトライ内で成功することを検証する（FR-3, FR-5）。\n- 前提条件: `maxRetries=3`, プロバイダモックが1回目に HTTP 429、2回目に成功レスポンスを返す。バックオフタイマはフェイクで制御。\n- 入力: 中優先度タスク1件、`IssueContext` あり。\n- 期待結果: `IssueAIGenerationResult.metadata.retryCount=1`、最終タイトル/本文は成功レスポンスを反映、WARN ログを出さず DEBUG ログのみで完了。\n- テストデータ: `task_medium_priority`, 成功レスポンス JSON。\n\n### テストケース名: issue_ai_generator_generate_invalid_json_異常系\n- 目的: プロバイダが JSON 以外のテキストを返す場合に `IssueAIValidationError` を送出しフォールバック条件となることを検証する（FR-2, FR-3）。\n- 前提条件: `enabled=true`、プロバイダモックが `\"**markdown only**\"` を返す。\n- 入力: タスク1件、`IssueContext` 任意。\n- 期待結果: `IssueAIValidationError` が throw され、呼び出し側でフォールバック処理に遷移できる。\n- テストデータ: `task_low_priority`, ノイズレスポンス `\"**markdown only**\"`.\n\n### テストケース名: issue_ai_generator_generate_missing_sections_異常系\n- 目的: 本文に必須セクションが不足している場合に検証エラーが発生することを確認する（FR-2）。\n- 前提条件: プロバイダが `## 実行内容` を欠いた本文を返す。\n- 入力: タスク1件、`IssueContext` 任意。\n- 期待結果: `IssueAIValidationError` が throw される。ログにバリデーション理由（missing sections）が WARN 出力される。\n- テストデータ: `task_high_priority`, レスポンス JSON から `## 実行内容` を意図的に削除。\n\n### テストケース名: issue_ai_generator_sanitize_payload_boundary_境界値\n- 目的: タスク数・文字数・配列要素数の上限とマスキング処理が正しく適用されることを検証する（FR-2, セキュリティ要件）。\n- 前提条件: 6件のタスク（高3/中2/低1）、長文の `steps`・`targetFiles`、Bearer トークン/メールアドレスを含む説明を用意。\n- 入力: `maxTasks=5`, `IssueContext` あり。\n- 期待結果: 高→中→低の優先度順に5件へ切り詰められる。文字列512文字でトリム済み。`targetFiles` は10件に制限。シークレット文字列が `[REDACTED_]` へ置換される。\n- テストデータ: `task_priority_set`, `context_with_secret`.\n\n### テストケース名: secret_masker_mask_object_正常系\n- 目的: `maskObject` がネスト/配列/循環参照を含むオブジェクトを破壊せずにマスキングすることを確認する（セキュリティ要件）。\n- 前提条件: `ignoredPaths=['tasks.1.meta']` を指定。循環参照を含むテストオブジェクトを作成。\n- 入力: API キー文字列、メールアドレス、Bearer トークンを含むオブジェクト。\n- 期待結果: 元オブジェクトは不変。戻り値で対象文字列が `[REDACTED_*]` に置換され、`ignoredPaths` 指定フィールドは未マスク。\n- テストデータ: `sanitization_fixture`.\n\n### テストケース名: issue_client_create_issue_llm_success_正常系\n- 目的: LLM 出力が成功した場合にタイトル/本文/メタデータが採用され、Octokit へ送信されることを検証する（FR-1〜FR-5）。\n- 前提条件: `appendMetadata=true`、`IssueAIGenerator` モックが成功結果を返す、Octokit モックが `issues.create` 呼び出しを記録。\n- 入力: タスク2件（高/中）、`IssueContext` あり。\n- 期待結果: Octokit へ渡るタイトル/本文が LLM 結果とメタデータ追記を含む。WARN ログは発生しない。\n- テストデータ: `task_high_priority`, `task_medium_priority`, `context_with_blocker`, LLM 成功レスポンス。\n\n### テストケース名: issue_client_create_issue_llm_fallback_異常系\n- 目的: LLM 失敗時に WARN ログと共に既存テンプレートへフォールバックすることを検証する（FR-3）。\n- 前提条件: `IssueAIGenerator.generate` が `IssueAIValidationError` を throw。Octokit モックが呼び出される。\n- 入力: タスク1件、`IssueContext` あり。\n- 期待結果: WARN ログ `FOLLOWUP_LLM_FALLBACK` が出力され、Octokit へはレガシータイトル/本文が送信される。\n- テストデータ: `task_low_priority`, 既存 `generateFollowUpTitle` で計算可能なキーワード。\n\n### テストケース名: issue_client_create_issue_llm_disabled_境界値\n- 目的: `IssueGenerationOptions.enabled=false` の場合に LLM を呼び出さず既存挙動を維持することを確認する（FR-4）。\n- 前提条件: `enabled=false`, `appendMetadata=false`。`IssueAIGenerator` モックは呼ばれていないことを検証。\n- 入力: タスク1件、`IssueContext` あり。\n- 期待結果: LLM 呼び出しが 0 回、Octokit へはレガシータイトル/本文が送信される。ログには LLM 無効化の INFO が出力される。\n- テストデータ: `task_medium_priority`.\n\n## 3. Integrationテストシナリオ\n\n### シナリオ名: CLIからIssueClientへのLLMオプション伝搬\n- 目的: CLI 引数・環境変数が PhaseContext を経由して `IssueGenerationOptions` に伝搬することを検証する（FR-4）。\n- 前提条件: OpenAI/Claude APIキーはダミー、`FOLLOWUP_LLM_MODE=auto` を設定。Octokit と LLM プロバイダはモック。\n- テスト手順:\n  1. `ai-workflow execute --issue 119 --phase evaluation --followup-llm-mode claude --followup-llm-model claude-3-sonnet-20240229 --followup-llm-timeout 20000 --followup-llm-max-retries 2 --followup-llm-append-metadata` を `NODE_ENV=test` で実行。\n  2. テストフックで `IssueClient.createIssueFromEvaluation` 呼び出し時の `options` をキャプチャ。\n  3. CLI 実行結果を検査。\n- 期待結果: 受け取った `options` が CLI 指定値（provider=claude, timeout=20000, maxRetries=2, appendMetadata=true, enabled=true）となる。フェイルオーバーは発動しない。\n- 確認項目: オプション値一致、ログに `FOLLOWUP_LLM_SUCCESS` が存在、Octokit 呼び出し成功。\n\n### シナリオ名: LLM失敗時のフォールバック統合動作\n- 目的: LLM 呼び出しがタイムアウトした場合に WARN ログと共にレガシーテンプレートへフォールバックする統合挙動を確認する（FR-3）。\n- 前提条件: LLM プロバイダモックが 3 回タイムアウト例外を投げる。`maxRetries=3`。Octokit/IssueAIGenerator 以外は実装通り。\n- テスト手順:\n  1. `npm run test:integration -- followup-issue-llm` を実行し、タイムアウトをシミュレート。\n  2. テスト内で WARN ログが出力されたかをアサート。\n  3. Issue 本文がレガシーテンプレートで生成されたか確認。\n- 期待結果: WARN ログ `FOLLOWUP_LLM_FALLBACK` が 1 件、Octokit へ送信された本文は `## 背景`・`## 残タスク詳細` を含む既存形式。テストは成功扱い。\n- 確認項目: WARN ログ内容、フォールバック本文、リトライ回数=3。\n\n### シナリオ名: 実APIエンドツーエンド検証（オプトイン）\n- 目的: 実際の LLM API 呼び出しで生成品質とセクション構造が満たされることを確認する（FR-1, FR-2, FR-5）。\n- 前提条件: `FOLLOWUP_LLM_E2E=1`, `ANTHROPIC_API_KEY` または `OPENAI_API_KEY` を設定。GitHub への書き込みはダミークライアントに差し替え（ネットワーク負荷を避ける）。\n- テスト手順:\n  1. `FOLLOWUP_LLM_E2E=1 npm run test:integration -- followup-issue-llm.e2e` を実行。\n  2. テストは LLM からの応答を取得し、タイトル長と本文セクションを検証。\n  3. 生成結果サンプルをスナップショットとして保存し、手動レビュー用に出力。\n- 期待結果: 50〜80文字のタイトルと 5 セクションを含む本文が生成される。`実行内容` に番号付きリストと「テスト」の記述を含む。メタデータが `durationMs` と `input/outputTokens` を保持。\n- 確認項目: タイトル文字数、各セクション存在、`appendMetadata` の有無、API呼び出し時間。\n\n## 4. テストデータ\n- `task_high_priority`:  \n  ```\n  {\n    task: \"core/gitカバレッジ向上\",\n    description: \"core/git モジュールの単体テストを追加しカバレッジ90%を目指す。\",\n    targetFiles: [\"src/core/git/index.ts\", \"src/core/git/utils.ts\"],\n    steps: [\"既存テストの重複を整理\", \"core/git に Jest テストを追加\", \"npm run test -- core/git\"],\n    acceptanceCriteria: [\"テストカバレッジレポートでcore/gitが90%を超える\"],\n    priority: \"HIGH\",\n    priorityReason: \"リリース前に品質基準を満たす必要がある\",\n    estimatedHours: 6\n  }\n  ```\n- `task_medium_priority`: 中優先度でステップ/ファイルが複数のタスク（`priority: \"MEDIUM\"`、`steps` 3件）。\n- `task_low_priority`: 低優先度で受け入れ基準が2件のタスク。\n- `task_priority_set`: 高3件・中2件・低1件を含む配列。1件に512文字超の説明と11件の `targetFiles` を設定。\n- `context_with_blocker`: `summary`, `blockerStatus`, `deferredReason` を含む IssueContext。`deferredReason` に「Bearer sk-test-abc12345 を含まないようマスキングが必要」と記述。\n- `context_with_secret`: `summary` にメールアドレス `owner@example.com` とアクセストークン `token=XYZ987654321` を含め、マスキングを検証。\n- `sanitization_fixture`: シークレット文字列、メールアドレス、Bearer トークン、循環参照を含むオブジェクト。\n- LLM モックレスポンス: 成功パターン JSON（タイトル70文字、本文5セクション）、検証失敗パターン（`## 実行内容` 欠落）、非 JSON 文字列パターン。\n\n## 5. テスト環境要件\n- Node.js 20.x / TypeScript 5.x、Jest ベースのテストランナー（`npm run test:unit`, `npm run test:integration`）。`ts-jest` もしくは `esbuild-jest` に対応。\n- LLM モック用に `nock` またはプロバイダアダプタの手動スタブを使用。タイマー制御には `jest.useFakeTimers()` を利用。\n- Octokit を呼び出す統合テストでは `@octokit/rest` をモックし、GitHub への実呼び出しを防止。\n- Optional 実APIテスト実行時のみネットワークアクセスと API キー（`ANTHROPIC_API_KEY` / `OPENAI_API_KEY`）を設定。CI では `FOLLOWUP_LLM_E2E` を未設定としてスキップ。\n- ログ検証のため `pino` などのロガーをテストダブルへ差し替え、WARN/DEBUG 出力をアサート可能にする。\n- テストデータは `fixtures/followup_llm/` 配下に JSON/TypeScript で管理し、共通ユーティリティでロードする。\n","exit_code":0,"status":"completed"}}
{"type":"item.updated","item":{"id":"item_0","type":"todo_list","items":[{"text":"Review planning, requirements, and design documents to understand scope and test strategy","completed":true},{"text":"Derive unit and integration test scenarios aligned with requirements and design","completed":true},{"text":"Draft and save test scenario markdown document at .ai-workflow/issue-119/03_test_scenario/output/test-scenario.md","completed":true}]}}
{"type":"item.completed","item":{"id":"item_16","type":"agent_message","text":"Produced the Phase 3 test scenario doc covering the UNIT_INTEGRATION strategy with detailed unit and integration cases plus data and environment needs; see `.ai-workflow/issue-119/03_test_scenario/output/test-scenario.md:1`. Scenarios validate LLM success, retries, validation failures, sanitization, SecretMasker, IssueClient fallback/disable flows, CLI option propagation, failure fallback, and optional live API checks, with shared fixtures and setup guidance called out in the same file at `.ai-workflow/issue-119/03_test_scenario/output/test-scenario.md:4`.\n\nNext steps:\n1. Review/adjust the scenario set as needed.\n2. Begin implementing the outlined unit tests, then integration tests.\n3. Plan for optional live LLM verification once API keys/configs are ready."}}
{"type":"item.completed","item":{"id":"item_0","type":"todo_list","items":[{"text":"Review planning, requirements, and design documents to understand scope and test strategy","completed":true},{"text":"Derive unit and integration test scenarios aligned with requirements and design","completed":true},{"text":"Draft and save test scenario markdown document at .ai-workflow/issue-119/03_test_scenario/output/test-scenario.md","completed":true}]}}
{"type":"turn.completed","usage":{"input_tokens":355470,"cached_input_tokens":319232,"output_tokens":8817}}